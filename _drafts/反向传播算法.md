---
title: 反向传播算法
tags: [机器学习, 深度学习, 反向传播]
categories: [深度学习,  反向传播] 
comments: true
toc: true
---
<img src="反向传播算法/反向传播算法首图.jpg" width="300" height="200" />

><font color=#0000FF face="微软雅黑" size=4>Nothing is perfect, nothing.</font>

***

## 一、引言
### 1.1 为什么写这个
&emsp;&emsp;上周末的时候，跟一位同样爱好深度学习/机器学习的同事交流，他跟我说反向传播算法这个知识点没看懂。当时我一琢磨，**反向传播算法(Back-Propagation Algorithm)**这块并没有涉及特别复杂的理论知识和数学计算，就一个[链式求导法则](https://en.wikipedia.org/wiki/Chain_rule)就搞定的东西，怎么会给他造成困扰？当时来不及细想，就给他推荐了几篇我认为写的比较好的资料。

&emsp;&emsp;回家之后我又仔细看了下给他推荐的这些资料，另外还参考了大量博客、书籍，大致猜到了他的困惑所在。要说起反向传播算法，相关的资料可以说是多如牛毛。但是这些资料，不管博客也好，书籍也好，要么过于学术化，光符号就是一大堆，要记住这些符号就不是一件容易的事，在证明的时候绕老绕去，自然就晕了。要么抄来抄去，画虎不成反类犬，很容易让初次接触者如坠云雾，造成一种吃不透的感觉。

&emsp;&emsp;本文的宗旨是，用更形象的类比、更精简的数学符号，把反向传播算法的精髓剖析出来。

### 1.2 以什么类比
&emsp;&emsp;其实很多算法的核心思想和我们人类的生活习惯、组织结构是非常相似的，原因很简单，人类不可能设计出超越自身认知的东西。

&emsp;&emsp;就反向传播算法而言，包括**前向传播算法(Forward propagation)**，它的思想和我们的企事业单位的工作原理几乎如出一辙。**本文也会以的运行机制做类比**，阐述前/后向传播算法的本质。

## 二、正文
### 2.1 神经网络结构

&emsp;&emsp;常见的神经网络结构如下图所示：

&emsp;&emsp;这个图也许是讲解神经网络用的最广泛的一个，大多数资料也都是大同小异。它是典型的输入层(Input Layer)  => 隐藏层(Hidden Layer) => 输出层(Output Layer)这样的三层复合结构。而隐藏层又是有$N, \ N \in [1, \infty)$层复合的，对于某些深度神经网络（比如何凯明提出的[深度残差网络](https://arxiv.org/pdf/1512.03385.pdf)）来说，这个$N$值可以高达上百。
### 2.1 NN *VS* 企业
&emsp;&emsp;我们来将常见的神经网络和企事业
