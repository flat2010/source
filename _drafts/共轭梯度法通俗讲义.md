---
title: 共轭梯度法通俗讲义
date: 2018-10-26 22:43:20
tags: [共轭]
categories: [数学理论] 
comments: true
toc: true
---
<img src="共轭梯度法通俗讲义/共轭梯度法通俗讲义首图.gif" width="350" height="250" />

><font color=#0000FF face="微软雅黑" size=4>Without the Agonizing Pain。</br><p align="right" style="color:black;font-size:12pt;">——Edition $1 \frac{1}{4}$ · August 4, 1994 · Jonathan Richard Shewchuk</p></font>

***

<p align="center" style="color:black;font-size:16pt;font-weight:bold;">摘   &emsp;  要</p>

&emsp;&emsp;共轭梯度法是稀疏线性方程组迭代求解法里面最优秀的方法。然而，大部分关于该算法的教科书即没有图示，讲解的也并不直观。因此，时至今日，仍然有许多这类教材的受害者在满是灰尘的图书馆角落里碎碎叨叨的胡诹一通。有鉴于此，几位睿智的精英们苦心孤诣的破解了前辈们留下的晦涩难懂的文字，并从几何角度深度的去阐释了这个算法。共轭梯度法本身也只是一个简单、优雅的复合方法。因此，睿智如你一定一学就会。

&emsp;&emsp;本文通过介绍[二次型（Quadratic Form）](https://en.wikipedia.org/wiki/Quadratic_form)，然后据此引出[最速下降（Steepest Descent）](https://en.wikipedia.org/wiki/Method_of_steepest_descent)、共轭方向以及共轭梯度。同时还对特征向量做了解释，并用于检验[雅可比方法（Jacobi Method）](https://en.wikipedia.org/wiki/Jacobi_method)、最速下降以及共轭梯度。此外，还包括预处理和非线性共轭梯度法的一些问题。为了使的本文易读易懂，我可谓是煞费苦心。本文廊括了66个图示，同时避免出现晦涩的词汇。同一个概念也分别用了几种不同的方式来解释，大多数等式都配有直观的解释说明。

<p align="left" style="color:black;font-size:12pt;font-weight:bold;">关键字：共轭梯度法，预处理，收敛性分析，通俗讲义</p>

</br></br>

<p align="center" style="color:black;font-size:10pt;">**版   &emsp;  权   &emsp;  申   &emsp;  明**</p>

&emsp;&emsp;<font size=2>本文原始文章版权归**Jonathan Richard Shewchuk(jrs@cs.cmu.edu)**所有，中文翻译版版权归本博客主人所有。**英文原版及翻译版均可随意复制、分发，但请务必完整保留英文原版及本条版权声明信息。**</font>

&emsp;&emsp;<font size=2>英文原版版权声明如下：</font>
>&emsp;&emsp;&copy;1994 by Jonathan Richard Shewchuk. This article may be freely duplicated and distributed so long as no consideration is received in return, and this copyright notice remains intact.
&emsp;&emsp;This guide was created to help students learn Conjugate Gradient Methods as easily as possible. Please mail me (jrs@cs.cmu.edu) comments, corrections, and any intuitions I might have missed; some of these will be incorporated into a second edition. I am particularly interested in hearing about use of this guide for classroom teaching.

&emsp;&emsp;<font size=2>对象要了解更多关于迭代算法的读者，我强烈推荐**William L.Briggs**所著的<font style="font-style:italic;" size=2>A Multigrid Tutorial</font> 即《多重网格法》，这是我读过的最好的数学教材之一。</font>

&emsp;&emsp;<font size=2>特别鸣谢**Omar Ghattas**，他给我讲了许多关于数值方法的知识，也给本文的初稿提出了许多宝贵的意见。同时还要感谢**James Epperson、David O'Hallaron、James Stichnoth、Nick Trefethen、Daniel Tunkelang**给本文提出的建议。</font>

&emsp;&emsp;<font size=2>为了帮助读者可以跳跃式的阅读本文，下面整理了一张章节之间的依赖关系图，如下所示：</font>

<img src="共轭梯度法通俗讲义/共轭梯度法通俗讲义章节关系图.svg" width="450" height="400" />
<div align='center' >图 1-1　　本文章节关系图<font color="red"></font></div> 

&emsp;&emsp;<font size=2>本文适用于每一位像我一样喜欢大量使用图示说明同时工于计算的人。</font>


<p align="center" style="color:black;font-size:16pt;">**目 录**</p>

### 1. 简介
&emsp;&emsp;当我决定学习共轭梯度法（简称**CG**，下同）时，读了有四种不同的关于该算法的描述文档，然而仍然分不清个子午卯酉，几乎是一无所获。这些文章大多数只是简单的“写”了下这个算法，然后对其特性做了推导证明。这些证明即没有任何直观的解释，也没有人提到CG算法的发明者灵感的来源。本文的诞生初衷正是源于本人在探索过程中所遭受的无数挫折，以期后人在学习CG算法的时候，学习的是一个全面、优雅的算法思想，而非一堆的公式证明。

&emsp;&emsp;CG是解决大型线性方程组问题最流行的迭代算法。它对于如下形式的问题特别有效：

$$
\textbf{A} \vec{x} = \vec{b}
\tag{1 - 1}
$$

&emsp;&emsp;式中，$\vec{x}$是我们要求解的未知向量，$\vec{b}$是一个已知的向量，$\textbf{A}$是一个对称的[正定](https://en.wikipedia.org/wiki/Positive-definite_matrix)方阵。如果你不记得什么是**正定矩阵**的话，没关系，我们在后面会对这个知识点进行回顾。上述等式应用的范围非常广，比如有限差分和有限元法求解偏微分方程、结构分析、电路分析以及数学作业。

&emsp;&emsp;像CG这样的迭代算法适合于稀疏矩阵。如果上式中的$\textbf{A}$是稠密的，最好的求解方法是先对$\textbf{A}$进行因式分解，然后用置换法回代求解。对稠密矩阵$\textbf{A}$进行因式分解所需要的时间大致与迭代求解方程的时间相同。一旦$\textbf{A}$分解完毕，即使存在多个不同的$b$的值，也能使用回代法快速的求解出方程组的解。

&emsp;&emsp;对比此稠密矩阵和一个规模更大但占用内存总量相同的稀疏矩阵，稀疏矩阵因式分解出来的三角阵的非零元素通常要比其原始矩阵的多。因式分解很多时候受限于内存大小将无法进行，并且耗时也非常久，即使是回代求解的过程也可能会比迭代求解法慢。换句话说，大多数的迭代算法在稀疏矩阵的求解上即节省内存、又高效快捷。

&emsp;&emsp;本文假定读者已经学过线性代数相关的课程，对矩阵乘法、线性无关有着深刻的理解（即使你现在对这些概念有些淡忘也没关系）。基于此，我方能为你清晰的构建CG知识体系的大厦。

### 2. 本文符号约定
&emsp;&emsp;我们从一些符号的定义和注释开始。
&emsp;&emsp;如未特别指出，本文符号及其表示内容如下：
- 1\. 大写字母表示矩阵；
- 2\. 小写字母表示向量；
- 3\. 希腊字母表示标量;
- 4\. $\vec{x}^T \cdot \vec{y} = \sum_{i=1}^{n} x_i y_i$表示两向量的内积，同时有$\vec{x}^T  \cdot \vec{y} = \vec{y}^T  \cdot \vec{x}$；
- 5\. $\vec{x}^T \cdot  \vec{y} = 0$表示向量$\vec x$与$\vec y$正交；
- 6\. $1 \times 1$的矩阵形如$\vec{x}^T \cdot  \vec{y}$以及$\vec{x}^T \textbf{A}  \vec{x}$视作标量。

&emsp;&emsp;假定$\textbf{A}$是一个$n \times n$的矩阵，$x、b$为向量（即$n \times 1$的矩阵），则有如下等式：

$$
\begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & A_{22} & \cdots & A_{2n} \\
\vdots &        & \ddots & \vdots\\
A_{n1} & A_{n2} & \cdots & A_{nn}
\end{bmatrix}
\begin{bmatrix}
x_{11}\\
x_{21}\\
\vdots\\
x_{n1}
\end{bmatrix} = 
\begin{bmatrix}
b_{1}\\
b_{2}\\
\vdots\\
b_{n}
\end{bmatrix}
\tag{2 - 1}
$$

&emsp;&emsp;所谓**正定矩阵**，是指对任意非零向量$\vec x$，恒满足如下不等式的矩阵：

$$
\vec{x}^T \textbf{A} \vec{x} > 0
\tag{2 - 2}
$$

&emsp;&emsp;这个解释可能对你来说还是过于抽象，从这个式子很难直观的看出所谓的正定矩阵和非正定矩阵的区别。别灰心，后面我们看到正定矩阵是如何影响二次型的形状的时候，就能对这个概念有个直观的理解了。

&emsp;&emsp;最后，别忘了这两个重要特性：
- 1\. $\textbf{AB}^T = \textbf{B}^T \textbf{A}^T$;
- 2\. $\textbf{AB}^{-1} = \textbf{B}^{-1} \textbf{A}^{-1}$.


### 3. 二次型
&emsp;&emsp;所谓的**二次型**，是关于向量的二次数值型函数，形如：

$$
f(\vec{x}) = \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \vec{b}^T \vec{x} + c
\tag{3 - 1}
$$

&emsp;&emsp;式中，$\textbf{A}$为一个矩阵，$\vec{x}、\vec{b}$为向量，$c$为一个常数。下面我先简单阐述一下，当$\textbf{A}$为**对称正定阵**的时候，$f(x)$的最小值由$\textbf{A} \vec{x} = \vec{b}$的解给出。

&emsp;&emsp;下面这个例子将贯穿本文：

$$
\textbf{A} = 
\begin{bmatrix}
3 & 2 \\
2 & 6
\end{bmatrix}, \quad \quad
b = 
\begin{bmatrix}
2 \\
-8
\end{bmatrix}, \quad \quad
c = 0
\tag{3 - 2}
$$

&emsp;&emsp;方程$\textbf{A} \vec{x} = \vec{b}$对应的图形如下所示：

<img src="共轭梯度法通俗讲义/二维线性方程组图示.png" width="450" height="400" />
<div align='center' >图 3-1　　二维线性方程组图示<font size=2 color="red">（方程组解为两直线交点）</font></div> 

&emsp;&emsp;更一般的，方程组的解$x$通常位于$n$维超平面（每一个是$n - 1$维）的交点处。就这个例子而言，解为：$\vec{x} = [2, \ -2]^T$。该例子对应的二次型的图像如下图所示：

<img src="共轭梯度法通俗讲义/二次型的示意图.png" width="450" height="400" />
<div align='center' >图 3-2　　二次型$f(\vec{x})$的示意图<font size=2 color="red">（曲面的最低点对应方程$\textbf{A} \vec{x} = b$的解）</font></div> 

&emsp;&emsp;因为矩阵$\textbf{A}$是正定的，因此其对应的二次型函数$f(\vec{x})$的图形像一个抛物型的碗，后面我们我作更详细的介绍。

&emsp;&emsp;二次型$f(\vec{x})$的等高线图如下所示：

<img src="共轭梯度法通俗讲义/二次型函数等高线图.png" width="450" height="400" />
<div align='center' >图 3-3　　二次型等高线示意图<font size=2 color="red">（每一条椭圆曲线对应一个固定的$f(\vec{x})$值）</font></div> 

&emsp;&emsp;二次型的梯度记作如下形式：

$$
f'(\vec{x}) = 
\begin{bmatrix}
\frac{\partial}{\partial_{x_1}} f(\vec{x}) \\
\frac{\partial}{\partial_{x_2}} f(\vec{x}) \\
\vdots \\
\frac{\partial}{\partial_{x_n}} f(\vec{x})
\end{bmatrix}
\tag{3 - 3}
$$

&emsp;&emsp;上述梯度实际上是[向量场](https://en.wikipedia.org/wiki/Vector_field)，对于任意给定的值$\vec{x}$，**梯度是使得二次型函数值$f(\vec{x})$增长最快的方向**。

&emsp;&emsp;<font color="black" size=1>译者注：这里的梯度是二次型的梯度，和我们平时所说的[普通多元函数的梯度](https://en.wikipedia.org/wiki/Gradient)有所区别。</font>

&emsp;&emsp;下图展示了上述二次型在给定式（3-2）的参数时的梯度向量：

<img src="共轭梯度法通俗讲义/二次型的梯度向量图.png" width="450" height="400" />
<div align='center' >图 3-4　　二次型梯度($f'(\vec{x})$)图<font size=2 color="red">（对每个$\vec{x}$，梯度指向$f(\vec{x})$增长最快速的方向）</font></div> 

&emsp;&emsp;结合图（3-2）、（3-4）可知，在图（3-2）的底部处，梯度为0，即图（3-4）中的小黑点。也就是说，将$f'(\vec {x})$设置为0并求解出对应的$\vec{x}$，我们就能求得二次型的最小值$f(\vec{x})_{min}$。

&emsp;&emsp;结合式（3-1）可以求出二次型的梯度为：

$$
f'(\vec{x}) = \frac{1}{2} \textbf{A}^T \vec{x} + \frac{1}{2} \textbf{A} \vec{x} - \vec{b}
\tag{3 - 4}
$$

<p align="left" style="color:gray;font-size:8pt;">&emsp;&emsp;译者注：这里的求解过程原文没有给出，这个过程也并不简单，为了便于线性代数较差的读者理解，现给出详细的求解步骤，以供参考。
&emsp;&emsp;考察式（3-3）中的任意项：

$$
\begin{split}
\frac{\partial}{\partial_{x_i}} f(\vec{x}) &= \frac{ \partial{ (\frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \vec{b}^T \vec{x} + c) } } {\partial_{x_i}} \\
&= \frac{1}{2} \cdot \frac{\partial{ (\vec{x}^T \textbf{A} \vec{x}) }} {\partial_{x_i}}    -     \frac{\partial{ (\vec{b}^T \vec{x}) }} {\partial_{x_i}}     +     \underbrace{ \frac{\partial{c}} {\partial_{x_i}} }_{0}
\end{split}
\tag{3 - 4 - 1}
$$

&emsp;&emsp;先来看上式中的第一项：

$$
\begin{split}
\frac{\partial{ (\overbrace{ \underbrace{\vec{x}^T \textbf{A}}_{g(\vec x)} \underbrace{\vec{x}}_{h(\vec x)}   }^{f_1(\vec{x})}) }} {\partial_{x_i}} &=   \underbrace{ \frac{\partial{ (\vec{x}^T \textbf{A} ) }}  {\partial_{x_i}} }_{g'(\vec x)}  \cdot \vec{x} + \vec{x}^T \textbf{A} \cdot \underbrace{  \frac{\partial{\vec{x} }} {\partial_{x_i}} }_{h'(\vec x)}
\end{split}
\tag{3 - 4 - 2}
$$

&emsp;&emsp;上式转换的依据是把$f_1(\vec{x}) = \vec{x}^T \textbf{A} \vec{x}$看作一个复合函数，由两个函数$g(\vec{x}) = \vec{x}^T \textbf{A}, \ h(\vec{x}) = \vec{x}$复合而成，即$f_1(\vec{x}) = g(\vec{x})  \cdot  h(\vec{x})$，再由复合函数的求导法则即为上式。

&emsp;&emsp;上式右边仍然是由两部分组成，我们仍然一项一项来求解。对于第一项：

$$
\begin{split}
\frac{\partial{ (\vec{x}^T \textbf{A} ) }}  {\partial_{x_i}} &= \frac{\partial}{\partial_{x_i}}  
\Bigg (
[x_1, x_2, \cdots , x_n]
\times
\begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & A_{22} & \cdots & A_{2n} \\
\vdots &        & \ddots & \vdots\\
A_{n1} & A_{n2} & \cdots & A_{nn}
\end{bmatrix}
\Bigg )
\\
&= \frac{\partial}{\partial_{x_i}} [ \underbrace{ x_1 \cdot A_{11} + \cdots + x_n \cdot A_{n1}, \ \cdots , \ x_1 \cdot A_{1n} + \cdots + x_n \cdot A_{nn} }_{1 \times n \quad vector} ]
\end{split}
\tag{3 - 4 - 3}
$$

&emsp;&emsp;再由向量对标量的求导法则（参见[这篇文章](https://flat2010.github.io/2017/05/12/%E6%A6%82%E5%BF%B5%E5%AE%9A%E4%B9%89%E6%9D%82%E8%AE%B0/)），上式最终变成：

$$
\begin{split}
\frac{\partial{ (\vec{x}^T \textbf{A} ) }}  {\partial_{x_i}} &= [ \frac{\partial}{\partial_{x_i}} (x_1 \cdot A_{11} + \cdots + x_n \cdot A_{n1} ), \ \cdots , \ \frac{\partial}{\partial_{x_i}}(x_1 \cdot A_{1n} + \cdots + x_n \cdot A_{nn}) ] \\
&= [A_{i1}, \ A_{i2}, \ \cdots, \ A_{in}]
\end{split}
\tag{3 - 4 - 4}
$$

&emsp;&emsp;于是第一项为：

$$
\begin{split}
\frac{\partial{ (\vec{x}^T \textbf{A} ) }}  {\partial_{x_i}}  \cdot \vec{x} &= [A_{i1}, \ A_{i2}, \ \cdots, \ A_{in}] \cdot  \vec{x} \\
&= A_{i1} \cdot x_1 + A_{i2} \cdot x_2 + \cdots + A_{in} \cdot x_n \\
&= \textbf{A}_{i, \*} \cdot \vec{x}
\end{split}
\tag{3 - 4 - 5}
$$

&emsp;&emsp;如上式中所示，我们用$\textbf{A}_{i, \*}$表示矩阵$\textbf{A}$的第$i$行的所有元素即$\textbf{A}$的第$i$个行向量。再来看式（3-4-2）的第二项，有：

$$
\begin{split}
\vec{x}^T \textbf{A} \cdot \frac{\partial{\vec{x}}} {\partial_{x_i}} &= \vec{x}^T \textbf{A} \cdot
[\frac{\partial{x_1}}{\partial{x_i}}, \ \cdots, \ \frac{\partial{x_n}}{\partial{x_i}}]^T \\
&= \vec{x}^T \textbf{A} \cdot [0, \ \cdots, \ \underbrace{1}_{  i^{th}  }, \ 0]^T \\
&= \vec{x}^T \bigg ( 
\begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & A_{22} & \cdots & A_{2n} \\
\vdots &        & \ddots & \vdots\\
A_{n1} & A_{n2} & \cdots & A_{nn}
\end{bmatrix}
\times 
\begin{bmatrix}
0 \\
\vdots \\
\underbrace{1}_{  i^{th}  } \\
0
\end{bmatrix}
\bigg ) \\
&= [x_1, x_2, \cdots , x_n]   \times  
\begin{bmatrix}
A_{1i} \\
A_{2i} \\
\vdots \\
A_{ni}
\end{bmatrix} \\
&= x_1 \cdot A_{1i} + x_2 \cdot A_{2i} + \cdots + x_n \cdot A_{ni} \\
&= \textbf{A}_{\*, i}^T \cdot \vec{x}
\end{split}
\tag{3 - 4 - 6}
$$

&emsp;&emsp;如上式中所示，我们用$\textbf{A}_{\*, i}$表示矩阵$\textbf{A}$的第$i$列的所有元素即$\textbf{A}$的第$i$个列向量。于是有：

$$
\frac{\partial{ (\vec{x}^T \textbf{A}} \vec{x})} {\partial_{x_i}} = \textbf{A}_{i, \*} \cdot \vec{x} + \textbf{A}_{\*, i}^T \cdot \vec{x}
\tag{3 - 4 - 7}
$$

&emsp;&emsp;我们再来看式（3-4-1）的第二项，同样根据向量对标量的求导法则有：

$$
\begin{split}
\frac{\partial{ (\vec{b}^T \vec{x}) }} {\partial_{x_i}} &= \frac{\partial}{\partial_{x_i}}
\bigg(
[b_1, \ b_2, \ \cdots, b_n] \times
\begin{bmatrix}
x_1 \\
\vdots \\
\underbrace{x_i}_{  i^{th}  } \\
\vdots \\
x_n
\end{bmatrix} 
\bigg) \\
&= \frac{\partial (b_1 \cdot x_1 + \cdots + b_i \cdot x_i + \cdots + b_n \cdot x_n)} {\partial_{x_i}} \\
&= b_i
\end{split}
\tag{3 - 4 - 8}
$$

&emsp;&emsp;综合上式（3-4-7）、（3-4-8）有：

$$
\begin{split}
\frac{\partial}{\partial_{x_i}} f(\vec{x}) &= \frac{1}{2} (\textbf{A}_{i, \*} \cdot \vec{x} + \textbf{A}_{\*, i}^T \cdot \vec{x}) + b_i \\
&= \frac{1}{2} \textbf{A}_{i, \*} \cdot \vec{x} + \frac{1}{2} \textbf{A}_{\*, i}^T \cdot \vec{x} - b_i
\end{split}
\tag{3 - 4 - 9}
$$

&emsp;&emsp;于是(3-3)有：

$$
\begin{split}
f'(\vec{x}) &= 
\begin{bmatrix}
\frac{1}{2} \textbf{A}_{1, \*} \cdot \vec{x} + \frac{1}{2} \textbf{A}_{\*, 1}^T \cdot \vec{x} - b_1 \\
\vdots \\
\frac{1}{2} \textbf{A}_{i, \*} \cdot \vec{x} + \frac{1}{2} \textbf{A}_{\*, i}^T \cdot \vec{x} - b_i \\
\vdots \\
\frac{1}{2} \textbf{A}_{n, \*} \cdot \vec{x} + \frac{1}{2} \textbf{A}_{\*, n}^T \cdot \vec{x} - b_n
\end{bmatrix} \\
&= \frac{1}{2} \begin{bmatrix}
 \textbf{A}_{1, \*} \cdot \vec{x} \\
\vdots \\
\textbf{A}_{i, \*} \cdot \vec{x} \\
\vdots \\
\textbf{A}_{n, \*} \cdot \vec{x} 
\end{bmatrix}  + 
\frac{1}{2} \begin{bmatrix}
\textbf{A}_{\*, 1}^T \cdot \vec{x} \\
\vdots \\
\textbf{A}_{\*, i}^T \cdot \vec{x} \\
\vdots \\
\textbf{A}_{\*, n}^T \cdot \vec{x}
\end{bmatrix}  - 
\begin{bmatrix}
b_1 \\
\vdots \\
b_i \\
\vdots \\
b_n
\end{bmatrix} \\
&= \frac{1}{2} \begin{bmatrix}
 \textbf{A}_{1, \*} \\
\vdots \\
\textbf{A}_{i, \*} \\
\vdots \\
\textbf{A}_{n, \*} 
\end{bmatrix} \cdot \vec{x} + \frac{1}{2} \begin{bmatrix}
\textbf{A}_{\*, 1}^T \\
\vdots \\
\textbf{A}_{\*, i}^T \\
\vdots \\
\textbf{A}_{\*, n}^T
\end{bmatrix} \cdot \vec{x}  - \vec{b} \\
&= \frac{1}{2} \textbf{A} \vec{x} + \frac{1}{2} \textbf{A}^T \vec{x} - \vec{b}
\end{split}
\tag{3 - 4 - 10}
$$

&emsp;&emsp;求解完毕！
</p>

&emsp;&emsp;若$\textbf{A}$为对称阵即有$\textbf{A}^T = \textbf{A}$，则式（3-4）可进一步化简为：

$$
f'(\vec{x}) = \textbf{A}^T \vec{x} - \vec{b}
\tag{3 - 5}
$$

&emsp;&emsp;再令上述梯度为0，即有我们要求解的线性方程（1-1）式。因此，通过上述的步骤，我们就把线性方程组$\textbf{A} \vec{x} = \vec{b}$的解和二次型函数图象的**驻点**联系起来了。如果$\textbf{A}$是对称正定阵，那么该驻点即为二次型函数$f(\vec{x})$的最小值点。因此，通过求解使得二次型函数$f(\vec{x})$值最小的点，我们就求出了线性方程组$\textbf{A} \vec{x} = \vec{b}$的解。

&emsp;&emsp;如果矩阵$\textbf{A}$为非对称阵，由于$\frac{1}{2}(\textbf{A}^T + \textbf{A})$为对称阵，因此(CG算法)让然能够求解出式（3-4）的解。

&emsp;&emsp;那么为何对称正定阵会有如此优良的特性呢？我们下面就来揭晓这个答案。考虑二次型函数$f(\vec{x})$曲面上的任意点$\vec{p}$以及最小值点$\vec{x} = \textbf{A}^{-1} \vec{b}$。将两个点$\vec{p}、\vec{x}$分别代入式（3-1），然后作差有如下结论：

$$
f(\vec{p}) = f(\vec{x}) + \frac{1}{2} (\vec{p} - \vec{x})^T \textbf{A} (\vec{p} - \vec{x}) 
\tag{3 - 5}
$$

&emsp;&emsp;关于上述结论困的证明，英文原版在文档的附录C1中给出了详细步骤，这里贴出我的证明思路，以供参考。

<p align="left" style="color:gray;font-size:8pt;">&emsp;&emsp;将两个点带入式（3-1），并且结合$\textbf{A} \vec{x} = \vec{b}$以及$\textbf{A}^T = \textbf{A}$，然后作差，有：

$$
\begin{split}
f(\vec{p}) - f(\vec{x}) &= (\frac{1}{2} \vec{p}^T \textbf{A} \vec{p} - \vec{b}^T \vec{x} + c) - (\frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \vec{b}^T \vec{x} + c)  \\
&= \frac{1}{2} \vec{p}^T \textbf{A} \vec{p} - \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - (\textbf{A} \vec{x})^T \vec{p} -  (\textbf{A} \vec{x})^T \vec{x} \\
&= \frac{1}{2} \vec{p}^T \textbf{A} \vec{p} - \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \vec{x}^T \textbf{A} \vec{p} -  \vec{x}^T \textbf{A} \vec{x} \\
&= \frac{1}{2} \vec{p}^T \textbf{A} \vec{p} + \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \vec{x}^T \textbf{A} \vec{p}
\end{split}
\tag{3 - 5 - 1}
$$

&emsp;&emsp;要进一步化简该式，还需要用到一个性质：若有矩阵$\textbf{A}^T = B$，且$\textbf{A}$为对称阵，则必有$\textbf{A} = \textbf{B}$。因为$(\vec{x}^T \textbf{A} \vec{p})^T = \vec{p} \textbf{A} \vec{x} $且$\vec{x}^T \textbf{A} \vec{p}$为标量（可视作$1 \times 1$的对称阵），所以必然有：$\vec{x}^T \textbf{A} \vec{p} = \vec{p} \textbf{A} \vec{x} $，于是上式可变成：

$$
\begin{split}
f(\vec{p}) - f(\vec{x}) &= \frac{1}{2} \vec{p}^T \textbf{A} \vec{p} + \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \frac{1}{2} \vec{x}^T \textbf{A} \vec{p} - \frac{1}{2} \vec{x}^T \textbf{A} \vec{p} \\
&= \frac{1}{2} \vec{p}^T \textbf{A} \vec{p} + \frac{1}{2} \vec{x}^T \textbf{A} \vec{x} - \frac{1}{2} \vec{x}^T \textbf{A} \vec{p} - \frac{1}{2} \vec{p} \textbf{A} \vec{x} \\
&= \frac{1}{2}(\vec{p}^T \textbf{A} \vec{p} - \vec{p} \textbf{A} \vec{x}) + \frac{1}{2}(\vec{x}^T \textbf{A} \vec{x} - \vec{x}^T \textbf{A} \vec{p}) \\
&= \frac{1}{2} \vec{p}^T \textbf{A} (\vec{p} - \vec{x}) + \frac{1}{2} \vec{x}^T \textbf{A} (\vec{x} - \vec{p}) \\
&= \frac{1}{2} (\vec{p}^T \textbf{A} - \vec{x}^T \textbf{A}) (\vec{p} - \vec{x}) \\
&= \frac{1}{2} (\vec{p}^T - \vec{x}^T) \textbf{A} (\vec{p} - \vec{x}) \\
&= \frac{1}{2} (\vec{p} - \vec{x})^T \textbf{A} (\vec{p} - \vec{x})
\end{split}
\tag{3 - 5 - 2}
$$

&emsp;&emsp;证毕！
</p>

&emsp;&emsp;若$\textbf{A}$为正定阵，则由式（2-2）可知，若$\vec{p} \neq \vec{x}$，则上式（3-5）中的第二项恒为正，即有：

$$
\frac{1}{2} (\vec{p} - \vec{x})^T \textbf{A} (\vec{p} - \vec{x}) > 0, \ \vec{p} \neq \vec{x}
\tag{3 - 6}
$$

&emsp;&emsp;所以进一步有：$f(\vec{p}) > f(\vec{x}), \ \vec{p} \neq \vec{x}$。即点$\vec{x}$为二次型函数$f(\vec{x})$的全局最小值点。这就解释了为何对称正定阵会有如此优良的特性。

&emsp;&emsp;因此，**对于正定阵最直观的理解方式就是它的二次型函数的图形是一个开口向上的抛物面。**如果矩阵$\textbf{A}$非正定，那么其全局最小值点就可能不止一个。

&emsp;&emsp;如果$\textbf{A}$为负定矩阵，则其图形刚好为其对应的正定阵抛物面上下翻转后的图形。如果$\textbf{A}$为奇异矩阵（此时线性方程组解不唯一），解集为一条直线或超平面。如果$\textbf{A}$不属于上面情况中的任何一种，则其解$\vec{x}$为一个[鞍点(Saddle Point)](https://en.wikipedia.org/wiki/Saddle_point)，此时无论是最速下降法还是共轭梯度法都无法求解出来。

&emsp;&emsp;下图展示了上面所说的这些情况的二次型函数的图形。注意，函数中$\vec{b}、c$的取值仅仅影响图形最小值点的位置，但不会影响图形的状。

<img src="共轭梯度法通俗讲义/不同情况下的二次型函数图.png" width="450" height="400" />
<div align='center' >图 3-5　　不同情况下的二次型函数图<font size=2 color="red"><br>(a) 正定矩阵的二次型函数图形。 (b) 负定矩阵二次函数图。 <br> (c) 正定奇异阵的函数图，过谷底的一条直线为解集。(d) [不定矩阵](http://mathworld.wolfram.com/IndefiniteMatrix.html)函数图 <br>对于三维及以上的情况，奇异矩阵也可能会存在鞍点。</font></div> 

&emsp;&emsp;我们为什么要把解线性方程组的问题转换成一个貌似更棘手的问题呢？原因在于，我们研究的最速下降法和共轭梯度法均是由图（3-2）所示的求最小化的问题创造出来的，它远比图（3-1）所示的超平面相交问题更直观易懂。

### 4. 最速下降法

### 5. 以特征向量和特征值的视角思考
#### 5.1 特征值尝试
#### 5.2 Jacobi迭代
#### 5.3 一个实例

### 6. 最速下降的收敛性分析
#### 6. 1 即时结果
#### 6.2 通用收敛性

### 7. 共轭方向法
#### 7.1 共轭
#### 7.2 格莱姆-施密特共轭
#### 7.3 误差项的最优性

### 8. 共轭梯度法

### 9. 共轭梯度法的收敛性分析
#### 9.1 选择最佳的多项式
#### 9.2 切比雪夫多项式

### 10. 复杂度

### 11. 起迄
#### 11.1 起
#### 11.2 迄

### 12. 预处理

### 13. 标准方程的共轭梯度

### 14. 非线性共轭梯度法
#### 14.1 非线性共轭梯度法概述
#### 14.2 通用线性搜索法
#### 14.3 预处理

### A. 后记

### B. 罐头算法
#### B1. 最速下降
#### B2. 共轭梯度
#### B3. 预处理共轭梯度
#### B4. 非线性共轭梯度与Newton-Raphson以及Fletcher-Reeves
#### B5. 预处理非线性共轭梯度与Secant以及Polak-Ribiere

### C. 丑陋的证明
#### C1. $Ax = b$的解是二次型的最小值
#### C2. 任意对称阵有$n$个正交特征向量
#### C3. 切比雪夫多项式的最优性

### 背景知识

