### LeNet

### 模式识别的变迁

&emsp;&emsp;以手写字符为例，传统的方法是手工设计特征提取器，现在是基于像素直接操作的自动学习器。

&emsp;&emsp;以文本理解为例，传统的方法是独立设计各个模块，现在是统一的、有良好设计范式的图像转换网络GTN(Graph Transformer Networks)。

#### 传统模式识别存在的问题

1. 手动设计的特征提取器与任务相关，包含了大量与特定任务有关的先验知识，泛化能力弱；
2. 大量丰富多变的自然数据，使得手工设计一套精准的识别系统已不可能；
3. 识别的精度主要取决于手工设计的特征；
4. 各个模块由于设计上相互独立，其模块参数先各自训练，然后整合到一起后，再重新统一训练，导致这个过程费时费力并且最后还可能不是最优解;
5. 无法抵抗旋转、缩放、扭曲等变形操作；

### 三点因素推动

1. 具有高速数学计算能力的、低廉的电脑的产生，对巧妙设计的算法的依赖性大大降低，更多的转向了“蛮力”的数值计算方法；
2. 大型数据库对具有广阔市场和广泛兴趣的问题（比如手写体识别问题）的有力支持，使得大量设计人员更多的依赖于真实的数据而非手工设计的特征来构建识别系统。
3. 第三点也是特别重要的一点是，强大的能处理高维输入并生成复杂决策函数的机器学习技术的兴起，使得我们能处理这些大型数据集。

### 论文内容概览

1. 手写字符识别(1、2章)；
2. 手写数字识别测试性能（3章）；
3. 从单字符识别到文章中单词、句子识别（4章）；
4. 经典启发式方法在单词识别或者其他字符串识别中存在的过度分割问题（5章）；
5. 基于梯度的判别及非判别方法在无需手动分词及标注的情况下训练单词级别的识别器技术。（6章）；
6. 空间置换(Spac-Displacement)神经网络通过在输入的所有可能位置扫描识别器消除了启发式分割的需要（7章）；
7. 基于一种统一的图像合成算法，可训练的GTN可以被公式化成多种广义转换；
8. 全局训练的GTN系统用于识别手写器的输入（在线手写体识别）；
9. 一套完全基于GTN的系统用于识别手写体及机打银行支票；



### 论文符号概览

- $Y^p = F（Z^p， W）​$：第$p​$轮迭代神经网络的输出（比如字符的类别）；
- $Z^p$：第$p$轮迭代神经网络的输入；
- $W$：系统可变参数（要学习的参数）；
- $D^p$：第$p$轮迭代时正确的输出；
- $E^p = \mathcal{D}（D^p， F（Z^p， W））$：第$p$轮迭代时的损失函数；
- $E_{train}(W)$：训练集上的平均损失函数；
- $H(w)$：正则化函数；



### 训练

1. 随机梯度下降；
2. 反向梯度传播；



### CNN

三个关键思想：

1. 局部感受野；
2. 权值共享（或权重复制）；
3. 空间或时间降采样。



- `feature map`：多个共享相同权重系数的单元组成的平面；
- 一个完整的卷积层的输出由多个权重系数不同的`feature maps`组成；
- 随着层数的递进，由于图片分辨率越来越低，因此`feature map`的数量越来越多；
- 降低图片的分辨率是为了应对输入的几何变换（旋转、平移等），而为了补偿这种分辨率的损失，则通过增加表示的丰富性（特征图数量）；

### LeNet

#### LeNet架构

&emsp;&emsp;LeNet架构概览图如下所示：

![LeNet-5架构图](LeNet论文阅读笔记/LeNet.png)

&emsp;&emsp;LeNet总共包含了 **8** 层（含输入层）：**1** 个输入层、 **3** 个卷积层、**2** 个池化/下采样层、**1**个全连接层、**1**个输出层组成。

#### 架构参数概览表

| 全局编号 | 名称 | 输入尺寸 |   核大小@步长@填充   | 输出尺寸 | 参数个数 |
| :---:  | :---: |  :---: |  :---:  |  :-----:  |  :-----:  |
| —— | 输入层 | * | —— | $32 \times 32$ |——|
| C1 | 第1个卷积层 | $32 \times 32$@1 | $5 \times 5$@1@VALID | $28 \times 28$@6 | 156 |
| S2 | 第1个池化/降采样层 | $28 \times 28$@6 | $2 \times 2$@2@VALID | $14 \times 14$@6 | 12 |
| C3 | 第2个卷积层 | $14 \times 14$@6 | $5 \times 5$@1@VALID | $10 \times 10$@16 | 1516 |
| S4 | 第2个池化/降采样层 | $10 \times 10$@16 | $2 \times 2$@2@VALID | $5 \times 5$@16 | 32 |
| C5 | 第3个卷积层 | $5 \times 5$@16 | $5 \times 5$@1@VALID | $1 \times 1$@120 | 48120 |
| F6 | 全连接层 | $1 \times 1$@120 | —— |  $1 \times 1$@84  | 10164 |
| —— | 输出层 | $1 \times 1$@84 |          ——          | * | —— |

#### 结构详解

##### 1. 输入层

&emsp;&emsp;会把任意尺寸的图片缩放为$32 \times 32$的标准化图片，然后提供给LeNet神经网络。这个尺寸要明显大于数据库中存放的手写字符的图片大小（以$28 \times 28$大小的图片为中心，其像素仅为$20 \times 20$）。之所以要这样做，是为了让一些潜在的区别性的特征（比如箭头的端点或边缘）能够出现在高维特征检测器的感受野的中心。

&emsp;&emsp;输入图像的背景像素（白色）被标准化为-0.1，前景像素（黑色）设置为1.175。这样输入的均值约等于0，方差则近似为1，这样有助于加速整个网络的训练。

##### 2. C1层
- 输入尺寸：$32 \times 32​$；
- 卷积核大小：$5 \times  5$；
- 卷积步长：1；
- 填充：不填充（VALID）；
- 卷积后尺寸：$\lfloor \frac{32 + 2*0 - 5}{1} + 1 \rfloor = 28​$；
- 特征图数量：6；
- 参数个数：$(\underbrace{5 \times 5}_{kernel\_size} + \underbrace{1}_{bias}) \times \underbrace{6}_{feature\_maps} = 156$；
- 连接数：$156 \times 28 \times 28 = 122304$（特征图上每一个像素有156个连接）。

##### 2. S2层
- 输入尺寸：$28 \times 28​$；
- 池化核大小：$2 \times  2$；
- 池化步长：2；
- 填充：不填充（VALID）；
- 池化后尺寸：$\lfloor \frac{28 + 2*0 - 2}{2} + 1 \rfloor = 14​$；
- 特征图数量：6；
- 参数个数：$(1 + \underbrace{1}_{bias}) \times \underbrace{6}_{feature\_maps} = 12$；
- 连接数：$(2 \times 2 + 1) \times 14 \times 14 \times 6 = 5880$（输出特征图的每个像素与原来的4个像素连接，再加上偏置）；
- 激活函数：sigmoid。

##### 3. C3层
&emsp;&emsp;C3层比较特殊，因为它的特征图由输入的**6**个剧增到了输出的**16**个，输出特征图的每个像素对应好几个输入的$5 \times 5$大小的单元。这16个输出特征图的合成关系如下图所示：

![LeNet-5架构图](LeNet论文阅读笔记/C3的特征图组合关系示意图.png)

&emsp;&emsp;由上图可知，对于输出特征图：

1. 编号0～5的特征图，每张的每个像素由**3**张输入特征图上$5 \times 5$大小的像素组合而成，即只连接3张输入特征图；
2. 编号6～14的特征图，则由**4**张输入特征图上$5 \times 5​$大小的像素组合而成。其中6～11是由**连续的4张**输入特征组合的，而12～14则是由**不连续的4张**输入特征图组合而成；
3. 编号15的特征图，则由所有**6**张的输入特征图上​$5 \times 5$大小的像素组合而成；

&emsp;&emsp;为何不把所有输出特征图的跟所有输入的特征图连接起来呢？原因有两个，第一个是控制连接数在一个合理的范围内。第二个也是最重要的一个，这样做能强制性的打破网络的对称性。

- 输入尺寸：$14 \times 14​$；
- 卷积核大小：$5 \times  5$；
- 卷积步长：1；
- 填充：不填充（VALID）；
- 卷积后尺寸：$\lfloor \frac{14 + 2*0 - 5}{1} + 1 \rfloor = 10​$；
- 特征图数量：16；
- 参数个数：$\underbrace{(5 \times 5 \times 3 + 1) \times 6}_{0-5号输出特征图} + \underbrace{(5 \times 5 \times 4 + 1) \times 9}_{6-14号输出特征图} + \underbrace{(5 \times 5 \times 6 + 1) \times 1}_{15号输出特征图}= 1516$；
- 连接数：$1516 \times 10 \times 10 = 151600$；

##### 4. S4层
- 输入尺寸：$10 \times 10$；
- 池化核大小：$2 \times  2$；
- 池化步长：2；
- 填充：不填充（VALID）；
- 池化后尺寸：$\lfloor \frac{10 + 2*0 - 2}{2} + 1 \rfloor = 5​$；
- 特征图数量：16；
- 参数个数：$(1 + \underbrace{1}_{bias}) \times \underbrace{16}_{feature\_maps} = 32​$；
- 连接数：$(2 \times 2 + 1) \times 5 \times 5 \times 16 = 2000$；
- 激活函数：sigmoid。

##### 5. C5层
&emsp;&emsp;C5层的特征图由输入的**16**个剧增到了输出的**120**个，输出特征图的每个像素对应所有输入特征图的$5 \times 5$大小的单元。这一层被标记为**卷积层**而非**全连接层**是因为，如果LeNet-5网络的输入图像变大，但其它参数保持不变，那么这一层输出的特征图则会比$1\times 1$这个尺寸大，因此就不是全连接层。

- 输入尺寸：即为输入层图像的大小，$5 \times 5​$；
- 卷积核大小：$5 \times  5$；
- 卷积步长：1；
- 填充：不填充（VALID）；
- 卷积后尺寸：$\lfloor \frac{5 + 2*0 - 5}{1} + 1 \rfloor = 1$；
- 特征图数量：120；
- 参数个数：$(5 \times 5 \times 16 + 1) \times 120 = 48120$；
- 连接数：$48120 \times 1 \times 1 = 48120$（这一层的参数个数等于链接数）；

##### 6. F6层
&emsp;&emsp;F6层为整个网络的全连接层，其输出的大小主要是根据输出层来确定的。
- 输入尺寸：$1 \times 1​$；
- 输出尺寸：$1 \times  1$；
- 输出数量：84；
- 参数个数：$(120 + 1) \times 84 = 10164​$；
- 连接数：$10164 \times 1 \times 1 = 10164$（这一层的参数个数也等于链接数）；