---
title: 概念定义杂记
date: 2017-05-12 12:32:57
tags: [数学概念, 数学定义]
categories: [数学理论] 
comments: true
toc: true
---
<img src="概念定义杂记/首图.png" width="350" height="300" />
><font color=#0000FF face="微软雅黑" size=4>Thinking deeply！</font>
***
{% blockquote %}
<font size=3 color="green">好记性不如烂笔头，专门开一篇来记录学习中遇到的一些重要概念。</font>    
{% endblockquote %}

## 一、高等数学
### 1. 函数空间
&emsp;&emsp; 看周志华的《机器学习》，里面有个名词函数空间，它的[中文维基百科](https://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0%E7%A9%BA%E9%97%B4)给出的定义是：
&emsp;&emsp; <font color='ff0000' >在数学中，函数空间是从集合X到集合Y的给定类型的**一组**函数。</font>
&emsp;&emsp; 通俗点来讲，这个“空间”中的所有元素都是**函数**(负责自变量到因变量的转换)，并且这些函数都满足一定的条件。
&emsp;&emsp; 函数空间的概念在许多数学领域中均有自己的意义，这里我们只关心集合论、线代中的相关问题。
<!-- more -->
#### 1.1 集合论
&emsp;&emsp; 在集合论中，从集合$X$到$Y$的函数空间记做：$X \rightarrow Y$或者$Y^X$，其中的特列就是，$X$的所有子集(**power set或powerset**)可以用从$X$到二项集合$\lbrace 0, 1 \rbrace$的所有函数的集合来标识，记做：$2^X$。

#### 1.2 线性代数
&emsp;&emsp; 在线性代数中，函数空间（也是向量空间）则是同一个域上的向量空间$V$到另一个向量空间$W$的所有**线性变换**的集合。


### 2. 偏导数
&emsp;&emsp; 在手写SVM证明过程的时候，发现偏导数的知识点远远不只我们平时高数里面学的那些。按照变量类型划分，有Scalar（标量）、Vector（矢量）、Matrix（矩阵），两两结合就有`3 * 3 = 9`中形式，如下表：

| 变量类型 |                    标量                    |                    向量                    |                    矩阵                    |
| :--: | :--------------------------------------: | :--------------------------------------: | :--------------------------------------: |
|  标量  | $\frac {\large \partial y} {\large \partial x}$ | $\large \frac {\partial \vec y} {\partial x}$ | $\frac {\large  \partial \textbf{Y}} {\large  \partial x}$ |
|  向量  | $\frac {\large \partial y} {\large \partial \vec x }$ | $\frac {\large \partial \vec y} {\large \partial \vec x }$ | $\color {fuchsia} {\frac {\large \partial \textbf{Y}} {\large \partial \vec x}}$ |
|  矩阵  | $\frac {\large \partial y} {\large \partial \textbf{X}}$ | $\color {fuchsia} {\frac {\large \partial \vec y} {\large \partial \textbf{X}}}$ | $\color {fuchsia} {\frac {\large \partial \textbf{Y}} {\large \partial \textbf{X}}}$ |

&emsp;&emsp;上述表中的偏导数有不同的记法，[wikipedia](https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors)上讲了两种，一种是**`Numerator-layout notation（分子记法）`**，另一种是**`Denominator-layout notation（分母记法）`**，当然也可以两种方式混排，为了减少节省篇幅（并且有些偏导仅能用分子记法表示），这里采用第一种**分子记法**。
&emsp;&emsp;注意，上表中带颜色部分的偏导，因为属于更高阶（秩≥2）的张量，无法直接写在一个矩阵中，所以在此不给出这部分表达式，wikipedia上的原话是：
>However, these derivatives are most naturally organized in a tensor of rank higher than 2, so that they do not fit neatly into a matrix. 

#### 2.1 标量的偏导

##### 2.1.1 标量对标量的偏导
&emsp;&emsp; 标量对标量的偏导（Scalar-by-scalar）的求法正是我们日常中见得最多也是用的最多的，如下：

$$
\frac {\partial \large y} {\partial x}
\tag{1 - 2 - 1}
$$

##### 2.1.2 向量对标量的偏导
&emsp;&emsp; 向量对标量偏导（Vector-by-scalar）求法如下：

$$
\frac {\partial \bf{\vec y}} {\partial x} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x} \\\\
\frac{\partial y_2}{\partial x} \\\\
\vdots \\\\
\frac{\partial y_m}{\partial x}
\end{bmatrix}
\tag{1 - 2 - 2}
$$

##### 2.1.3 矩阵对标量的偏导
&emsp;&emsp; 矩阵对标量偏导（Matrix-by-scalar）求法如下，其中矩阵$\bf{X}$为`m * n`矩阵：

$$
\frac {\partial \large \textbf{Y}} {\partial x} = 
\begin{bmatrix}
 \frac{\partial y\_{11}}{\partial x}  &  \frac{\partial y\_{12}}{\partial x}   &  \cdots  &  \frac{\partial y\_{1n}}{\partial x} \\\\
\frac{\partial y\_{21}}{\partial x}   &  \frac{\partial y\_{22}}{\partial x}  &   \cdots  &  \frac{\partial y\_{2n}}{\partial x} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
 \frac{\partial y\_{m1}}{\partial x}  &  \frac{\partial y\_{m2}}{\partial x}  &  \cdots  &  \frac{\partial y\_{mn}}{\partial x}
\end{bmatrix}
\tag{1 - 2 - 3}
$$

#### 2.2 向量的偏导
##### 2.2.1 标量对向量的偏导
&emsp;&emsp; 标量对向量偏导（Scalar-by-vector）求法如下：

$$
\frac {\partial \large y} {\partial \bf{\vec x}} =
\begin{bmatrix}
\frac {\partial \large y} {\partial  x\_1} \ , \ \frac {\partial \large y} {\partial  x\_2} \ , \ \cdots \ , \ \frac {\partial \large y} {\partial  x\_n} 
\end{bmatrix}
\tag{1 - 2 - 4}
$$


##### 2.2.2 向量对向量的偏导
&emsp;&emsp; 向对向量偏导（Vector-by-vector）求法如下：

$$
\frac {\partial \large \bf{\vec y}} {\partial \large \bf{\vec x}} = 
\begin{bmatrix}
\frac{\partial y\_1}{\partial x_1} & \frac{\partial y\_1}{\partial x\_2} & \cdots & \frac{\partial y\_1}{\partial x\_n} \\\\
\frac{\partial y\_2}{\partial x\_1} & \frac{\partial y\_2}{\partial x\_2} & \cdots & \frac{\partial y\_2}{\partial x\_n} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
\frac{\partial y\_m}{\partial x\_1} & \frac{\partial y\_m}{\partial x\_2} & \cdots & \frac{\partial y\_m}{\partial x\_n}
\end{bmatrix}
\tag{1 - 2 - 5}
$$

##### 2.2.3 *矩阵对向量的偏导
&emsp;&emsp; 矩阵对向量偏导（Matrix-by-vector）在此不予列出，原因前文已述及

#### 2.3 矩阵的偏导
##### 2.3.1 标量对矩阵的偏导
&emsp;&emsp; 标量对矩阵偏导（Scalar-by-Matrix）求法如下，其中矩阵$\bf{X}$为`m * n`矩阵：

$$
\frac {\partial \large y} {\partial \large \bf{Y}} = 
\begin{bmatrix}
\frac{\partial y}{\partial x\_{11}} & \frac{\partial y}{\partial x\_{21}} & \cdots & \frac{\partial y}{\partial x\_{m1}} \\\\
\frac{\partial y}{\partial x\_{12}} & \frac{\partial y}{\partial x\_{22}} & \cdots & \frac{\partial y}{\partial x\_{m2}} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
\frac{\partial y}{\partial x\_{1n}} & \frac{\partial y}{\partial x\_{2n}} & \cdots & \frac{\partial y}{\partial x\_{mn}} \\\\
\end{bmatrix}
\tag{1 - 2 - 6}
$$

##### 2.3.2 *向量对矩阵的偏导
&emsp;&emsp; 向量对矩阵偏导（Vector-by-Matrix）在此也不予列出。

##### 2.3.3 *矩阵对矩阵的偏导
&emsp;&emsp; 矩阵对矩阵偏导（Matrix-by-Matrix）在此也不予列出。

### 3. 概率分布
#### 3.1 多元/维高斯分布
##### 3.1.1 一元/维高斯分布
&emsp;&emsp;设$X \sim N(\mu, \sigma^2)$，则其概率密度函数为：

$$
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} exp \lgroup -\frac{(x - \mu)^2}{2 \sigma^2} \rgroup
\tag{1 - 3 - 1}
$$

&emsp;&emsp;式中$\mu$为期望，$\sigma$为方差。

##### 3.1.2 二元/维高斯分布
&emsp;&emsp;设$X\_1 \sim N(\mu\_1, \sigma^2\_1), \ X\_2 \sim N(\mu\_2, \sigma^2\_2)$，且$X\_1、X\_2$并不相互独立，其相关系数为$\rho$，即有$(X\_1, \ X\_2) \sim (\mu\_1, \ \mu\_2, \ \sigma^2\_1, \ \sigma^2\_2, \ \rho)$，则其概率密度函数为：


$$
\begin{split}
f(x\_1, \ x\_2) = \frac{1}{2 \pi \sigma\_1 \sigma\_2 \sqrt{1 - \rho^2}}    exp \lbrack &-\frac{1}{2(1 - \rho^2)}    \lgroup \frac{(x\_1 - \mu\_1)^2}{\sigma^2\_1}    \  \\\\
&- \  \frac{2\rho (x\_1 - \mu\_1)(x\_2 - \mu\_2)}{\sigma\_1 \sigma\_2}   \ \\\\
&+ \ \frac{(x\_2 - \mu\_2)^2}{\sigma^2\_2}  \rgroup  \rbrack
\end{split}
\tag{1 - 3 - 2}
$$

&emsp;&emsp;改写成向量形式，现记：

$$
\begin{split}
\vec x &= (x\_1, \ x\_2)^T \\\\
\vec \mu &= (\mu\_1, \ \mu\_2)^T \\\\
C &= 
\begin{bmatrix} 
\sigma^2\_1 & \rho \sigma\_1 \sigma\_2  \\\\
\rho \sigma\_1 \sigma\_2 & \sigma^2\_2
\end{bmatrix}
 = 
 \begin{bmatrix} 
c\_{11} & c\_{12}  \\\\
c\_{21} & c\_{22}
\end{bmatrix}
\end{split}
$$

&emsp;&emsp;则二元/维高斯分布概率密度的向量形式为：

$$
f(\vec x) = f(x\_1, \ x\_2) = \frac{1}{2\pi \sqrt{det (C) }} exp \lbrack -\frac{1}{2} (\vec x - \vec \mu)^T C^{-1} (\vec x - \vec \mu) \rbrack
\tag{1 - 3 - 3}
$$

&emsp;&emsp;式中，$det(C) = |C| = \sigma^2\_1 \sigma^2\_2 - \rho^2 \sigma^2\_1 \sigma^2\_2 = \sigma^2\_1 \sigma^2\_2(1 - \rho^2)$，为**协方差矩阵$C$**对应的行列式之值。如果对上式(3 - 3)的指数中的幂有疑问，自行展开对比即可，这里不赘述。

##### 3.1.3 多元/维高斯分布
&emsp;&emsp;从二元/维高斯分布出发，可以推导出（过程略）$n$元/维高斯分布的向量形式为：

$$
f(\vec x; \mu \ , C) = \frac{1}{(2\pi)^{\frac{n}{2}} (det (C))^\frac{n}{2}} exp \lbrack -\frac{1}{2} (\vec x - \vec \mu)^T C^{-1} (\vec x - \vec \mu) \rbrack
\tag{1 - 3 - 4}
$$

&emsp;&emsp;式中$C$为一$n*n$的协方差阵，$det(C)$仍然表示其对应的行列式的值。


## 二、信息论
### 1.1 采样
&emsp;&emsp;根据奈奎斯特（香农采样定理）采样定理：

$$
f\_s > 2· f\_{max}
\tag{2 - 1 - 1}
$$

&emsp;&emsp;用语言描述则是：为了准确还原原始信号，采样频率要大于**信号中最高频率**的2倍。公式非常简单，但是其所蕴含的哲理却并不那么显见，特别是**信号中的最高频率**这几个字非常值得体会。
&emsp;&emsp;通信理论中，采样分过采样(oversampling) $VS$ 欠采样(undersampling)、上采样(upsampling) $VS$ 下采样(downsampling)。

### 1.1.1 过采样
&emsp;&emsp;过采样，是指**采样频率**<font color="blue">大于</font>**信号频率**<font color="red">2倍及以上</font>的采样，只有过采样才能重建原始信号。过采样针对**模拟信号**到**数字信号**的过程。

### 1.1.2 欠采样
&emsp;&emsp;欠采样与过采样是一对，顾名思义，它是指**采样频率**<font color="blue">小于</font>**信号频率**<font color="red">2倍</font>的采样，欠采样无法完全重建原始信号。欠采样也是针对**模拟信号**到**数字信号**的过程。
&emsp;&emsp;欠采样的一个比较著名的现象是告诉旋转的车轮看起来更像是在反着转，这种现象称为**混叠**。

### 1.1.3 上采样
&emsp;&emsp;上采样，是通过插值的方式对原始信号进行处理（比如图像的放大）。

### 1.1.4 下采样
&emsp;&emsp;下采样，它与上采样是一对，它通过采样的方式来减少原始数据（比如图像的缩小）。比如对一副分辨率为$M \* N$的图片，进行$k$倍下采样后，得到的图像分辨率为$(M/k) \* (N/k)$。

## 三、矩阵论
### 1 范数
&emsp;&emsp;关于范数，[Wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics))上式这么定义的：

> In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space—save for the zero vector, which is assigned a length of zero. A seminorm, on the other hand, is allowed to assign zero length to some non-zero vectors (in addition to the zero vector).

&emsp;&emsp;范数满足三个特性：
- 1.$p(\vec x) \geq 0$，即非负性；
- 2.$p(a \vec x) = |a| \cdot p(\vec x)$，即齐次性；
- 2.$p(\vec x + \vec y) \leq p(\vec x) + p(\vec y)$，即三角不等式；

&emsp;&emsp;其中$p(\cdot)$表示对$\cdot$取范数。范数分为向量**范数(vector norm)**和**矩阵范数(matrix norm)**。

#### 1.1 向量范数
##### 1.1.1 向量$p$范数
&emsp;&emsp;向量的p范数定义为**向量中所有元素绝对值的$p$次方之和再开$p$次方**。

$$
||\vec x||_{p} = (\sum_{i=1}^N |x_i|^p)^{\frac{1}{p}}, \quad p \in [1,  \ +\infty] \ 或 \ p = 0
\tag{3 - 1 - 1}
$$

&emsp;&emsp;根据$p$最常用的取值，我们有以下几种范数（注意当$p \in (0, 1)$的时候并不满足范数的三角不等式特性）。
##### 1.1.2 向量0范数
&emsp;&emsp;向量的0范数定义为**向量中所有非零元素的个数**。

##### 1.1.3 向量1范数
&emsp;&emsp;向量的1范数定义为**向量中所有元素的绝对值之和**。

$$
||\vec x||_1 = \sum_{i=1}^N |x_i|
\tag{3 - 1 - 2}
$$

##### 1.1.4 向量2范数
&emsp;&emsp;向量的2范数定义为**向量中所有元素平方之和再开方**，向量的二范数实际上与欧式距离是一样的。

$$
||\vec x||_2 = \sqrt{\sum_{i=1}^N x_i^2}
\tag{3 - 1 - 3}
$$

##### 1.1.5 向量$\infty$范数
&emsp;&emsp;向量的2范数定义为**向量中所有元素绝对值的最大值**。

$$
||\vec x||_{\infty} = \max_{i=1} |x_i|
\tag{3 - 1 - 4}
$$

#### 1.2 矩阵范数
##### 1.2.1 矩阵$p$范数
&emsp;&emsp;矩阵的$p$范数定义为：

$$
||\textbf{A}||_{p} = (\max_{||x||_p = 1} ||\textbf{A}x||_p)^{\frac{1}{p}}
\tag{3 - 1 - 5}
$$

##### 1.2.2 矩阵1范数
&emsp;&emsp;矩阵的1范数定义为**矩阵每一行的所有列元素绝对值之和的最大值，故又称为列和范数**：

$$
||\textbf{A}||_{1} = \max_{j} \sum_{i} |\textbf{A}_{ij}|
\tag{3 - 1 - 6}
$$

##### 1.2.3 矩阵2范数
&emsp;&emsp;矩阵的2范数定义为**矩阵最大特征值的开方，又称普范数**：

$$
||\textbf{A}||_{2} = \sqrt{\max_{j} \lambda_j} = \sqrt{\max eig(A^H A)}
\tag{3 - 1 - 7}
$$

##### 1.2.4 矩阵$\infty$范数
&emsp;&emsp;矩阵的$\infty$范数定义为**矩阵每一列所有行元素绝对值之和的最大值，故又称为行和范数**：

$$
||\textbf{A}||_{infty} = \max_{i} \sum_{j} |\textbf{A}_{ij}|
\tag{3 - 1 - 7}
$$

##### 1.2.5 矩阵$F$范数
&emsp;&emsp;矩阵的$F$(**Frobenius**)范数定义为**矩阵所有元素绝对值平方之和再开方**：

$$
||\textbf{A}||_{F} = \sqrt{\sum_{ij} |\textbf{A}_{ij}|^2} = \sqrt{Tr(A A^H)}
\tag{3 - 1 - 8}
$$

##### 1.2.6 矩阵**Max**范数
&emsp;&emsp;矩阵的max范数定义为**矩阵所有元素绝对值的最大值**：

$$
||\textbf{A}||_{max} = \max_{ij} |\textbf{A}_{ij}|
\tag{3 - 1 - 9}
$$

##### 1.2.7 矩阵**KF**范数
&emsp;&emsp;矩阵的KF范数定义为**矩阵奇异值的1范数**：

$$
||\textbf{A}||_{KF} = ||sing(\textbf{A})||_1
\tag{3 - 1 - 10}
$$

&emsp;&emsp;上式中$||sing(\textbf{A})||$表示矩阵**A**的奇异值向量。

### 2 特殊矩阵
&emsp;&emsp;一些比较有意思的矩阵。
#### 2.1 Toeplitz矩阵
&emsp;&emsp;[托普利兹矩阵](https://en.wikipedia.org/wiki/Toeplitz_matrix)(Toeplitz)，简称为**T矩阵**，又称**常对角矩阵**，定义为如下形式：

$$
\textbf{A} = 
\begin{bmatrix}
   a_0 & a_{-1} & a_{-2} &\cdots & \cdots & a_{-(n-1)}\\
   a_1 & a_0 & a_{-1} &\ddots & \ddots & \vdots \\
   a_2 & a_1 & a_{0} &\ddots & \ddots & \vdots \\
   \vdots & \ddots & \ddots &\ddots & a_{-1} & a_{-2} \\
   \vdots & \ddots & \ddots &a_1 & a_0 & a_{-1} \\
   a_{n-1} & \cdots & \cdots &a_2 & a_1 & a_0 \\
\end{bmatrix}
\tag{3 - 2 - 1}
$$

&emsp;&emsp;Toeplitz矩阵的特点是其第$i$行$j$列的元素，等于其左上角即$i-1$行$j - 1$列的元素，数学表达式为：

$$
\textbf{A}_{i,j} = \textbf{A}_{i-1,j-1}
\tag{3 - 2 - 2}
$$

&emsp;&emsp;Toeplitz矩阵其元素完全由第1行和第1列的$2n-1$个元素就能确定。它之所以叫常对角矩阵，是因为**其左上至右下的每一条对角线上的所有元素都是相同的**，T矩阵**没有要求必须是方阵**。

##### 2.1.1 Toeplitz矩阵示例1
&emsp;&emsp;下面是一个数字信号处理领域经常用到的特殊T型矩阵，它不但具备T型阵的特点，还是一个对称阵，如下所示：

$$
\textbf{T} = 
\begin{bmatrix}
   t_0 & t_1 & t_2 &\cdots & t_n\\
   t_1 & t_0 & t_1 &\cdots & t_{n-1}\\
   t_2 & t_1 & t_0 &\cdots & t_{n-2}\\
   \vdots & \vdots & \vdots &\ddots & \vdots\\
   t_n & t_{n-1} & t_{n-2} &\cdots & t_0\\
\end{bmatrix}
\tag{3 - 2 - 3}
$$

##### 2.1.2 Toeplitz矩阵示例2
&emsp;&emsp;再来，下面是一个前向移位矩阵：

$$
\textbf{T} = 
\begin{bmatrix}
   0 & 1 &\cdots & 0\\
   0 & \ddots & \ddots &\vdots\\
   \vdots & \ddots & \ddots &1\\
   0 & \cdots & 0 &0\\
\end{bmatrix}
\tag{3 - 2 - 4}
$$

&emsp;&emsp;Toeplitz矩阵的另一个重要特性是可以用来**做卷积！**

$$
y = h*x = 
\begin{bmatrix}
   h_1 & 0 & \cdots & 0 & 0\\
   h_2 & h_1 & \cdots & \vdots & \vdots\\
   h_3 & h_2 & h_1 & \cdots & 0\\
   \vdots & \vdots & \vdots &\ddots & \vdots\\
   0 & 0 & \cdots & h_{m-1} & h_{m-2}\\
   \vdots & \vdots & \vdots & h_{m} & h_{m-1}\\
   0 & 0 & 0 & \cdots & h_{m}\\
\end{bmatrix}
\begin{bmatrix}
   x_1\\
   x_2\\
   x_3\\
   \vdots\\
   x_n\\
\end{bmatrix}
\tag{3 - 2 - 5}
$$

&emsp;&emsp;上式的另一种表达形式为：

$$
y^T = 
\begin{bmatrix}
   h_1 & h_2 &h_3 & \cdots &h_n\\
\end{bmatrix}
\begin{bmatrix}
   x_1 & x_2 & x_3 & \cdots & x_n & 0 & 0 & 0 & \cdots & 0\\
   0 & x_1 & x_2 & x_3 & \cdots & x_n & 0 & 0 & \cdots & 0\\
   0 & 0 & x_1 & x_2 & x_3 & \cdots & x_n & 0 & \cdots & 0\\
   \vdots & \vdots & \vdots &\vdots & \vdots & \cdots & \vdots & \vdots & \cdots & 0\\
   0 & \cdots & 0 & 0 & x_1 & \cdots & x_{n-2} & x_{n-1} & x_n & \vdots\\
   0 & \cdots & 0 & 0 & 0 & x_1 & \cdots & x_{n-2} & x_{n-1} & x_n\\
\end{bmatrix}
\tag{3 - 2 - 6}
$$

&emsp;&emsp;一些比较有意思的矩阵。
#### 2.2 Hankel矩阵
&emsp;&emsp;[汉克尔矩阵](https://en.wikipedia.org/wiki/Hankel_matrix)(Hankle)，是**方阵**，其每一条副对角线上的元素均相等，定义为如下形式：

$$
\textbf{A} = 
\begin{bmatrix}
   a_0 & a_1 & a_2 &\cdots & \cdots & a_{(n-1)}\\
   a_1 & a_2 & a_3 &\ddots & \ddots & \vdots \\
   a_2 & \vdots & \vdots &\ddots & \ddots & \vdots \\
   \vdots & \ddots & \ddots &\ddots & \ddots & a_{2n-4} \\
   \vdots & \ddots & \ddots &\ddots & a_{2n-4} & a_{2n-3} \\
   a_{n-1} & \cdots & \cdots &a_{2n-4} & a_{2n-3} & a_{2n-2} \\
\end{bmatrix}
\tag{3 - 2 - 7}
$$

&emsp;&emsp;Hankle矩阵的特点是其第$i$行$j$列的元素，等于对称位置即$j$行$i$列的元素，数学表达式为：

$$
\textbf{A}_{i,j} = \textbf{A}_{j,i}
\tag{3 - 2 - 8}
$$

&emsp;&emsp;Hankle矩阵其是一个**对称阵**，它与上面所说来的Toeplitz矩阵有着紧密联系，仔细观察可以发现，Hankle矩阵**自右上至左下的每一条对角线的所有元素均是相同的，刚好与T矩阵相反**。

&emsp;&emsp;<font color="red">若$H \in C^{n \times n}，T \in C^{n \times n}$即H、T分别表示$n \times n$的Hankle、Toeplit方阵，那么它们之间的相互转换关系为：</font>

$$
\begin{cases}
\begin{split}
\textbf{T} &= \textbf{J}_n \cdot \textbf{H} = \textbf{H} \cdot \textbf{J}_n \\
\\
\textbf{H} &= \textbf{J}_n \cdot \textbf{T} = \textbf{T} \cdot \textbf{J}_n \\
\end{split}
\end{cases}
\tag{3 - 2 - 9}
$$

&emsp;&emsp;对于非方阵，则有：

$$
\textbf{H}_{m \times n} = \textbf{T}_{m \times n} \cdot \textbf{J}_n \\
\tag{3 - 2 - 10}
$$

&emsp;&emsp;式中$\textbf{J}_n$表示n阶反向方阵，即：

$$
\textbf{J}_n = 
\begin{bmatrix}
   0 & 0 & 0 &\cdots & 0 & 1\\
   0 & 0 & 0 &\ddots & 1 & 0 \\
   \vdots & \vdots & \vdots &\vdots & \vdots & \vdots \\
   0 & 1 & 0 &\cdots & 0 & 0 \\
   1 & 0 & 0 & \cdots & 0 & 0 \\
\end{bmatrix}
\tag{3 - 2 - 11}
$$

#### 2.2.1 Hankel转Toeplitz示例
&emsp;&emsp;给定Hankel、Toeplitz矩阵如下：

$$
\textbf{T}_{5 \times 5} = 
\begin{bmatrix}
   a & b & c &d & e\\
   f & a & b & c &d\\
   g & f & a & b & c\\
   h & g & f & a & b\\
   i & h & g & f & a\\
\end{bmatrix} \quad , \quad
\textbf{H}_{5 \times 5} = 
\begin{bmatrix}
   a & b & c &d & e\\
   b & c & d & e &f\\
   c & d & e &f & g\\
   d & e &f & g & h\\
   e &f & g & h & i\\
\end{bmatrix}
\tag{3 - 2 - 12}
$$

&emsp;&emsp;则$\textbf{T}_{5 \times 5} \cdot \textbf{J}_{5 \times 5}$结果如下：

$$
\textbf{T} \cdot \textbf{J} = 
\begin{bmatrix}
   a & b & c &d & e\\
   f & a & b & c &d\\
   g & f & a & b & c\\
   h & g & f & a & b\\
   i & h & g & f & a\\
\end{bmatrix} \cdot 
\begin{bmatrix}
   0 & 0 & 0 &0 & 1\\
   0 & 0 & 0 & 1 &0\\
   0 & 0 & 1 & 0 & 0\\
   0 & 1 & 0 & 0 & 0\\
   1 & 0 & 0 & 0 & 0\\
\end{bmatrix} =
\begin{bmatrix}
   e & d & c &b & a\\
   d & c & b & a &f\\
   c & b & a & f & g\\
   b & a & f & g & h\\
   a & f & g & h & i\\
\end{bmatrix}
\tag{3 - 2 - 13}
$$

&emsp;&emsp;把上式最终结果和$\textbf{H}_{5 \times 5}$对比，<font color="red">虽然元素上没有严格对应（比如前者副对角线上元素是$a$，而后者是$e$），但是前者形式上已经完全符合Hankle矩阵的定义。Wiki上提到的两者的关系，指的就是这个意思，而并不是说元素要严格一致，我之前因为这个原因纠结了很久。</font>

&emsp;&emsp;附上Wiki关于两个矩阵关系的原话：

> The Hankel matrix is closely related to the Toeplitz matrix (a Hankel matrix is an upside-down Toeplitz matrix). 