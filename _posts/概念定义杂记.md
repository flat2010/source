---
title: 概念定义杂记
date: 2017-05-12 12:32:57
tags: [数学概念, 数学定义]
categories: [数学理论] 
comments: true
toc: true
---
<img src="概念定义杂记/首图.png" width="350" height="300" />
><font color=#0000FF face="微软雅黑" size=4>Thinking deeply！</font>
***
{% blockquote %}
<font size=3 color="green">好记性不如烂笔头，专门开一篇来记录学习中遇到的一些重要概念。</font>    
{% endblockquote %}

## 一、高等数学
### 1. 函数空间
&emsp;&emsp; 看周志华的《机器学习》，里面有个名词函数空间，它的[中文维基百科](https://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0%E7%A9%BA%E9%97%B4)给出的定义是：
&emsp;&emsp; <font color='ff0000' >在数学中，函数空间是从集合X到集合Y的给定类型的**一组**函数。</font>
&emsp;&emsp; 通俗点来讲，这个“空间”中的所有元素都是**函数**(负责自变量到因变量的转换)，并且这些函数都满足一定的条件。
&emsp;&emsp; 函数空间的概念在许多数学领域中均有自己的意义，这里我们只关心集合论、线代中的相关问题。
<!-- more -->
#### 1.1 集合论
&emsp;&emsp; 在集合论中，从集合$X$到$Y$的函数空间记做：$X \rightarrow Y$或者$Y^X$，其中的特列就是，$X$的所有子集(**power set或powerset**)可以用从$X$到二项集合$\lbrace 0, 1 \rbrace$的所有函数的集合来标识，记做：$2^X$。

#### 1.2 线性代数
&emsp;&emsp; 在线性代数中，函数空间（也是向量空间）则是同一个域上的向量空间$V$到另一个向量空间$W$的所有**线性变换**的集合。


### 2. 偏导数
&emsp;&emsp; 在手写SVM证明过程的时候，发现偏导数的知识点远远不只我们平时高数里面学的那些。按照变量类型划分，有Scalar（标量）、Vector（矢量）、Matrix（矩阵），两两结合就有`3 * 3 = 9`中形式，如下表：

| 变量类型 |                    标量                    |                    向量                    |                    矩阵                    |
| :--: | :--------------------------------------: | :--------------------------------------: | :--------------------------------------: |
|  标量  | $\frac {\large \partial y} {\large \partial x}$ | $\large \frac {\partial \vec y} {\partial x}$ | $\frac {\large  \partial \textbf{Y}} {\large  \partial x}$ |
|  向量  | $\frac {\large \partial y} {\large \partial \vec x }$ | $\frac {\large \partial \vec y} {\large \partial \vec x }$ | $\color {fuchsia} {\frac {\large \partial \textbf{Y}} {\large \partial \vec x}}$ |
|  矩阵  | $\frac {\large \partial y} {\large \partial \textbf{X}}$ | $\color {fuchsia} {\frac {\large \partial \vec y} {\large \partial \textbf{X}}}$ | $\color {fuchsia} {\frac {\large \partial \textbf{Y}} {\large \partial \textbf{X}}}$ |

&emsp;&emsp;上述表中的偏导数有不同的记法，[wikipedia](https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors)上讲了两种，一种是**`Numerator-layout notation（分子记法）`**，另一种是**`Denominator-layout notation（分母记法）`**，当然也可以两种方式混排，为了减少节省篇幅（并且有些偏导仅能用分子记法表示），这里采用第一种**分子记法**。
&emsp;&emsp;注意，上表中带颜色部分的偏导，因为属于更高阶（秩≥2）的张量，无法直接写在一个矩阵中，所以在此不给出这部分表达式，wikipedia上的原话是：
>However, these derivatives are most naturally organized in a tensor of rank higher than 2, so that they do not fit neatly into a matrix. 

#### 2.1 标量的偏导

##### 2.1.1 标量对标量的偏导
&emsp;&emsp; 标量对标量的偏导（Scalar-by-scalar）的求法正是我们日常中见得最多也是用的最多的，如下：

$$
\frac {\partial \large y} {\partial x}
\tag{1 - 2 - 1}
$$

##### 2.1.2 向量对标量的偏导
&emsp;&emsp; 向量对标量偏导（Vector-by-scalar）求法如下：

$$
\frac {\partial \bf{\vec y}} {\partial x} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x} \\\\
\frac{\partial y_2}{\partial x} \\\\
\vdots \\\\
\frac{\partial y_m}{\partial x}
\end{bmatrix}
\tag{1 - 2 - 2}
$$

##### 2.1.3 矩阵对标量的偏导
&emsp;&emsp; 矩阵对标量偏导（Matrix-by-scalar）求法如下，其中矩阵$\bf{X}$为`m * n`矩阵：

$$
\frac {\partial \large \textbf{Y}} {\partial x} = 
\begin{bmatrix}
 \frac{\partial y\_{11}}{\partial x}  &  \frac{\partial y\_{12}}{\partial x}   &  \cdots  &  \frac{\partial y\_{1n}}{\partial x} \\\\
\frac{\partial y\_{21}}{\partial x}   &  \frac{\partial y\_{22}}{\partial x}  &   \cdots  &  \frac{\partial y\_{2n}}{\partial x} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
 \frac{\partial y\_{m1}}{\partial x}  &  \frac{\partial y\_{m2}}{\partial x}  &  \cdots  &  \frac{\partial y\_{mn}}{\partial x}
\end{bmatrix}
\tag{1 - 2 - 3}
$$

#### 2.2 向量的偏导
##### 2.2.1 标量对向量的偏导
&emsp;&emsp; 标量对向量偏导（Scalar-by-vector）求法如下：

$$
\frac {\partial \large y} {\partial \bf{\vec x}} =
\begin{bmatrix}
\frac {\partial \large y} {\partial  x\_1} \ , \ \frac {\partial \large y} {\partial  x\_2} \ , \ \cdots \ , \ \frac {\partial \large y} {\partial  x\_n} 
\end{bmatrix}
\tag{1 - 2 - 4}
$$


##### 2.2.2 向量对向量的偏导
&emsp;&emsp; 向对向量偏导（Vector-by-vector）求法如下：

$$
\frac {\partial \large \bf{\vec y}} {\partial \large \bf{\vec x}} = 
\begin{bmatrix}
\frac{\partial y\_1}{\partial x_1} & \frac{\partial y\_1}{\partial x\_2} & \cdots & \frac{\partial y\_1}{\partial x\_n} \\\\
\frac{\partial y\_2}{\partial x\_1} & \frac{\partial y\_2}{\partial x\_2} & \cdots & \frac{\partial y\_2}{\partial x\_n} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
\frac{\partial y\_m}{\partial x\_1} & \frac{\partial y\_m}{\partial x\_2} & \cdots & \frac{\partial y\_m}{\partial x\_n}
\end{bmatrix}
\tag{1 - 2 - 5}
$$

##### 2.2.3 *矩阵对向量的偏导
&emsp;&emsp; 矩阵对向量偏导（Matrix-by-vector）在此不予列出，原因前文已述及

#### 2.3 矩阵的偏导
##### 2.3.1 标量对矩阵的偏导
&emsp;&emsp; 标量对矩阵偏导（Scalar-by-Matrix）求法如下，其中矩阵$\bf{X}$为`m * n`矩阵：

$$
\frac {\partial \large y} {\partial \large \bf{Y}} = 
\begin{bmatrix}
\frac{\partial y}{\partial x\_{11}} & \frac{\partial y}{\partial x\_{21}} & \cdots & \frac{\partial y}{\partial x\_{m1}} \\\\
\frac{\partial y}{\partial x\_{12}} & \frac{\partial y}{\partial x\_{22}} & \cdots & \frac{\partial y}{\partial x\_{m2}} \\\\
\vdots  &  \vdots  &  \ddots  &  \vdots \\\\
\frac{\partial y}{\partial x\_{1n}} & \frac{\partial y}{\partial x\_{2n}} & \cdots & \frac{\partial y}{\partial x\_{mn}} \\\\
\end{bmatrix}
\tag{1 - 2 - 6}
$$

##### 2.3.2 *向量对矩阵的偏导
&emsp;&emsp; 向量对矩阵偏导（Vector-by-Matrix）在此也不予列出。

##### 2.3.3 *矩阵对矩阵的偏导
&emsp;&emsp; 矩阵对矩阵偏导（Matrix-by-Matrix）在此也不予列出。

### 3. 概率分布
#### 3.1 多元/维高斯分布
##### 3.1.1 一元/维高斯分布
&emsp;&emsp;设$X \sim N(\mu, \sigma^2)$，则其概率密度函数为：

$$
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} exp \lgroup -\frac{(x - \mu)^2}{2 \sigma^2} \rgroup
\tag{1 - 3 - 1}
$$

&emsp;&emsp;式中$\mu$为期望，$\sigma$为方差。

##### 3.1.2 二元/维高斯分布
&emsp;&emsp;设$X\_1 \sim N(\mu\_1, \sigma^2\_1), \ X\_2 \sim N(\mu\_2, \sigma^2\_2)$，且$X\_1、X\_2$并不相互独立，其相关系数为$\rho$，即有$(X\_1, \ X\_2) \sim (\mu\_1, \ \mu\_2, \ \sigma^2\_1, \ \sigma^2\_2, \ \rho)$，则其概率密度函数为：


$$
\begin{split}
f(x\_1, \ x\_2) = \frac{1}{2 \pi \sigma\_1 \sigma\_2 \sqrt{1 - \rho^2}}    exp \lbrack &-\frac{1}{2(1 - \rho^2)}    \lgroup \frac{(x\_1 - \mu\_1)^2}{\sigma^2\_1}    \  \\\\
&- \  \frac{2\rho (x\_1 - \mu\_1)(x\_2 - \mu\_2)}{\sigma\_1 \sigma\_2}   \ \\\\
&+ \ \frac{(x\_2 - \mu\_2)^2}{\sigma^2\_2}  \rgroup  \rbrack
\end{split}
\tag{1 - 3 - 2}
$$

&emsp;&emsp;改写成向量形式，现记：

$$
\begin{split}
\vec x &= (x\_1, \ x\_2)^T \\\\
\vec \mu &= (\mu\_1, \ \mu\_2)^T \\\\
C &= 
\begin{bmatrix} 
\sigma^2\_1 & \rho \sigma\_1 \sigma\_2  \\\\
\rho \sigma\_1 \sigma\_2 & \sigma^2\_2
\end{bmatrix}
 = 
 \begin{bmatrix} 
c\_{11} & c\_{12}  \\\\
c\_{21} & c\_{22}
\end{bmatrix}
\end{split}
$$

&emsp;&emsp;则二元/维高斯分布概率密度的向量形式为：

$$
f(\vec x) = f(x\_1, \ x\_2) = \frac{1}{2\pi \sqrt{det (C) }} exp \lbrack -\frac{1}{2} (\vec x - \vec \mu)^T C^{-1} (\vec x - \vec \mu) \rbrack
\tag{1 - 3 - 3}
$$

&emsp;&emsp;式中，$det(C) = |C| = \sigma^2\_1 \sigma^2\_2 - \rho^2 \sigma^2\_1 \sigma^2\_2 = \sigma^2\_1 \sigma^2\_2(1 - \rho^2)$，为**协方差矩阵$C$**对应的行列式之值。如果对上式(3 - 3)的指数中的幂有疑问，自行展开对比即可，这里不赘述。

##### 3.1.3 多元/维高斯分布
&emsp;&emsp;从二元/维高斯分布出发，可以推导出（过程略）$n$元/维高斯分布的向量形式为：

$$
f(\vec x; \mu \ , C) = \frac{1}{(2\pi)^{\frac{n}{2}} (det (C))^\frac{n}{2}} exp \lbrack -\frac{1}{2} (\vec x - \vec \mu)^T C^{-1} (\vec x - \vec \mu) \rbrack
\tag{1 - 3 - 4}
$$

&emsp;&emsp;式中$C$为一$n*n$的协方差阵，$det(C)$仍然表示其对应的行列式的值。


## 二、信息论
### 1.1 采样
&emsp;&emsp;根据奈奎斯特（香农采样定理）采样定理：

$$
f\_s > 2· f\_{max}
\tag{2 - 1 - 1}
$$

&emsp;&emsp;用语言描述则是：为了准确还原原始信号，采样频率要大于**信号中最高频率**的2倍。公式非常简单，但是其所蕴含的哲理却并不那么显见，特别是**信号中的最高频率**这几个字非常值得体会。
&emsp;&emsp;通信理论中，采样分过采样(oversampling) $VS$ 欠采样(undersampling)、上采样(upsampling) $VS$ 下采样(downsampling)。

### 1.1.1 过采样
&emsp;&emsp;过采样，是指**采样频率**<font color="blue">大于</font>**信号频率**<font color="red">2倍及以上</font>的采样，只有过采样才能重建原始信号。过采样针对**模拟信号**到**数字信号**的过程。

### 1.1.2 欠采样
&emsp;&emsp;欠采样与过采样是一对，顾名思义，它是指**采样频率**<font color="blue">小于</font>**信号频率**<font color="red">2倍</font>的采样，欠采样无法完全重建原始信号。欠采样也是针对**模拟信号**到**数字信号**的过程。
&emsp;&emsp;欠采样的一个比较著名的现象是告诉旋转的车轮看起来更像是在反着转，这种现象称为**混叠**。

### 1.1.3 上采样
&emsp;&emsp;上采样，是通过插值的方式对原始信号进行处理（比如图像的放大）。

### 1.1.4 下采样
&emsp;&emsp;下采样，它与上采样是一对，它通过采样的方式来减少原始数据（比如图像的缩小）。比如对一副分辨率为$M \* N$的图片，进行$k$倍下采样后，得到的图像分辨率为$(M/k) \* (N/k)$。

## 三、矩阵论
### 1 范数
&emsp;&emsp;关于范数，[Wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics))上式这么定义的：

> In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space—save for the zero vector, which is assigned a length of zero. A seminorm, on the other hand, is allowed to assign zero length to some non-zero vectors (in addition to the zero vector).

&emsp;&emsp;范数满足三个特性：
- 1.$p(\vec x) \geq 0$，即非负性；
- 2.$p(a \vec x) = |a| \cdot p(\vec x)$，即齐次性；
- 2.$p(\vec x + \vec y) \leq p(\vec x) + p(\vec y)$，即三角不等式；

&emsp;&emsp;其中$p(\cdot)$表示对$\cdot$取范数。范数分为向量**范数(vector norm)**和**矩阵范数(matrix norm)**。

#### 1.1 向量范数
##### 1.1.1 向量$p$范数
&emsp;&emsp;向量的p范数定义为**向量中所有元素绝对值的$p$次方之和再开$p$次方**。

$$
||\vec x||_{p} = (\sum_{i=1}^N |x_i|^p)^{\frac{1}{p}}, \quad p \in [1,  \ +\infty] \ 或 \ p = 0
\tag{3 - 1 - 1}
$$

&emsp;&emsp;根据$p$最常用的取值，我们有以下几种范数（注意当$p \in (0, 1)$的时候并不满足范数的三角不等式特性）。
##### 1.1.2 向量0范数
&emsp;&emsp;向量的0范数定义为**向量中所有非零元素的个数**。

##### 1.1.3 向量1范数
&emsp;&emsp;向量的1范数定义为**向量中所有元素的绝对值之和**。

$$
||\vec x||_1 = \sum_{i=1}^N |x_i|
\tag{3 - 1 - 2}
$$

##### 1.1.4 向量2范数
&emsp;&emsp;向量的2范数定义为**向量中所有元素平方之和再开方**，向量的二范数实际上与欧式距离是一样的。

$$
||\vec x||_2 = \sqrt{\sum_{i=1}^N x_i^2}
\tag{3 - 1 - 3}
$$

##### 1.1.5 向量$\infty$范数
&emsp;&emsp;向量的2范数定义为**向量中所有元素绝对值的最大值**。

$$
||\vec x||_{\infty} = \max_{i=1} |x_i|
\tag{3 - 1 - 4}
$$

#### 1.2 矩阵范数
##### 1.2.1 矩阵$p$范数
&emsp;&emsp;矩阵的$p$范数定义为：

$$
||\textbf{A}||_{p} = (\max_{||x||_p = 1} ||\textbf{A}x||_p)^{\frac{1}{p}}
\tag{3 - 1 - 5}
$$

##### 1.2.2 矩阵1范数
&emsp;&emsp;矩阵的1范数定义为**矩阵每一行的所有列元素绝对值之和的最大值，故又称为列和范数**：

$$
||\textbf{A}||_{1} = \max_{j} \sum_{i} |\textbf{A}_{ij}|
\tag{3 - 1 - 6}
$$

##### 1.2.3 矩阵2范数
&emsp;&emsp;矩阵的2范数定义为**矩阵最大特征值的开方，又称普范数**：

$$
||\textbf{A}||_{2} = \sqrt{\max_{j} \lambda_j} = \sqrt{\max eig(A^H A)}
\tag{3 - 1 - 7}
$$

##### 1.2.4 矩阵$\infty$范数
&emsp;&emsp;矩阵的$\infty$范数定义为**矩阵每一列所有行元素绝对值之和的最大值，故又称为行和范数**：

$$
||\textbf{A}||_{infty} = \max_{i} \sum_{j} |\textbf{A}_{ij}|
\tag{3 - 1 - 7}
$$

##### 1.2.5 矩阵$F$范数
&emsp;&emsp;矩阵的$F$(**Frobenius**)范数定义为**矩阵所有元素绝对值平方之和再开方**：

$$
||\textbf{A}||_{F} = \sqrt{\sum_{ij} |\textbf{A}_{ij}|^2} = \sqrt{Tr(A A^H)}
\tag{3 - 1 - 8}
$$

##### 1.2.6 矩阵**Max**范数
&emsp;&emsp;矩阵的max范数定义为**矩阵所有元素绝对值的最大值**：

$$
||\textbf{A}||_{max} = \max_{ij} |\textbf{A}_{ij}|
\tag{3 - 1 - 9}
$$

##### 1.2.7 矩阵**KF**范数
&emsp;&emsp;矩阵的KF范数定义为**矩阵奇异值的1范数**：

$$
||\textbf{A}||_{KF} = ||sing(\textbf{A})||_1
\tag{3 - 1 - 10}
$$

&emsp;&emsp;上式中$||sing(\textbf{A})||$表示矩阵**A**的奇异值向量。