---
title: 拉格朗日对偶问题
date: 2017-07-02 23:35:37
tags: [拉格朗日, 对偶问题]
categories: [数学理论, 高等数学]
comments: true 
toc: true
---


<img src="拉格朗日对偶问题/首图.png" width="350" height="200" />
><font color=#0000FF face="微软雅黑" size=4>Lagrange multiplier.</font>

### 1. 概述
#### 1.1 优化问题
&emsp;&emsp;<font color=#000000 size=5>**生**</font>活中总是存在着各种各样的最优化问题，比如出行的时候，不坐地铁的情况下（地铁贵啊！）如何坐车时间最省。工作上时间一定的情况下，如何安排任务能最快的完成。这些问题就是数学里面的约束最优化问题，按照约束类型和约束条件的不同，可以分成以下三大类：
<!--  more -->
&emsp;&emsp;1、零约束（无约束）最优化；
$$
\min \limits\_{} { f(x) } \ \ or \ \ \max \limits\_{} { f(x) } 
$$

&emsp;&emsp;2、一个及以上<font color='red'>**等式约束**</font>最优化（其中$h\_i(x)$表示第i个约束条件）；
$$
\min \limits\_{} { f(x) } \ \ or \ \ \max \limits\_{} { f(x) } \\\\
s.t \ \ h_i(x) = 0，i = 1, 2, ···, n
$$

&emsp;&emsp;3、一个及以上<font color='red'>**不等式约束**</font>最优化（同时还可以有等式约束）；
$$
\min \limits\_{} { f(x) } \ \ or \ \ \max \limits\_{} { f(x) } \\\\
\begin{split}
\textbf{s.t}\ \ h_i(x) & \geq 0（或h_i(x) \leq 0），i = 1，2，···，n \\\\
g_j(x) &= 0，j = 0，1，···，m
\end{split}
$$

#### 1.2 本文约定
&emsp;&emsp;为了叙述方便，我们仅对两种最优化问题（最大或最小）中的一种进行探讨，另一种其证明过程和原理是完全相同的，如无特殊说明，下文中的最优化问题都是指对目标函数求<font color='red'>**最小值**</font>，即：
$$
\min \limits\_{} { f(x) }
$$
#### 1.3 优化求解
&emsp;&emsp;针对以上三种优化问题，对应的求解方法如下：
##### 1.3.1 Fermat引理
&emsp;&emsp;对于第一种情况（零约束优化），高数里面我们就学过，通过求解函数的偏导数（partial derivative）并令其为零，即对函数$f(x\_1, x\_2, ..., x\_n)$，分别另$\frac{\partial f}{\partial x\_i} = 0$即可求得函数的所有**极值点**，再结合Fermat引理：

- $f''(x_0) < 0，x_0为极大值$；
- $f''(x_0) > 0，x_0为极小值$；
- $f''(x_0) = 0，无法判断（可改用列表法）$。

&emsp;&emsp;即可求出最值。

##### 1.3.2 Lagrange Multiplier
&emsp;&emsp;对于第二种情况（带等式约束优化），实际上在高中的时候我们就学过，通过增加拉格朗日系数，构造出拉格朗日函数，并对各个变量求偏导后令其为零并结合约束方程联解方程组，从而求解出各个变量的候选解集，最后回代并验证这些解集，最后求出最优值。这种方法就是广为人知的拉格朗日乘子法（Lagrange Multiplier）！
&emsp;&emsp;举例来说，对含$m$个等式约束$h_i(x) = 0，i = 1, 2, ···, m$的$n$元目标函数$f(x_1, x_2, ..., x_n)$，其拉格朗日函数为：

$$
L(x,\alpha) = f(x) + { \sum\_{i=1}^m \alpha\_i  h\_i(x) }
$$

&emsp;&emsp;对$n$个**优化变量$x_i$**分别求偏导后等于零的等式以及原约束条件等式组成的方程组为：

$$
\begin{cases}
\begin{split}
\frac {\partial L} {\partial x_1} = 0 \\\\
··· \ ··· \\\\
\frac {\partial L} {\partial x_n} = 0 \\\\
h_1(x) = 0 \\\\
··· \ ···\\\\
h_m(x) = 0 \\\\
\end{split}
\end{cases}
$$

&emsp;&emsp;上述方程组一共有$m + n$个待求解变量（$m$个$\alpha\_i$，$n$个$x_i$），也有$m + n$个方程，于是便能解出$\alpha\_i、x_i$，也就求出了所有可能的极值点。
&emsp;&emsp;实际上，含等式约束的最优化问题可以转换为不含等式约束的最优化问题。**因为约束条件$h_i(x)=0$等于拉格朗日函数$L(x,\alpha)$对第$i$个拉格朗日乘子$\alpha\_i$的偏导数！**即有：

$$
h_i(x) = \frac {\partial L(x, \alpha)} {\partial \alpha_i}
$$

&emsp;&emsp;我们把拉格朗日乘子扩展为目标函数的**优化变量**：

$$
f(x_1, x_2, ..., x_n) \ ==> \ f(x_1, x_2, ..., x_n, \alpha_1, ..., \alpha_m)
$$

&emsp;&emsp;这样等式约束的优化问题的求解就和**`1.3.1`**节中不带等式约束的优化问题一模一样了！**带不等式约束的优化问题同样可以转换为不带约束的优化问题**，这个下面会讲。
&emsp;&emsp;有时候只需要稍稍改变一下视角，你就能看到一个全新的世界。
##### 1.3.3 Lagrange Multiplier + KKT Conditions
&emsp;&emsp;对于第三种情况（带不等式约束优化），可能大多数非数学专业的同学都没有接触过，需要结合KKT（Karush–Kuhn–Tucker conditions）条件（KKT Conditions）来求解，这里就涉及到一个新的概念——KKT条件。为了更方便排版叙述，下面新开一个小节来说说这个KKT条件。

#### 1.4 KKT条件
##### 1.4.1 概述
&emsp;&emsp;KKT条件的全称是Karush–Kuhn–Tucker  conditions(也称Kuhn–Tucker conditions)，多应用于数学中的优化问题（非线性最优化）。
&emsp;&emsp;KKT条件的作用就在于：它给出了判断优化问题中某个解$x^\*$是否为最优解的**必要条件！**
&emsp;&emsp;考虑如下的非线性（带不等式约束）最优化问题：

$$
\begin{split}
目标&函数：f(x) \\\\
约束&条件： \\\\
\ \ &g_i(x) \leq 0 \\\\
\ \ &h_j(x) = 0
\end{split}
$$

&emsp;&emsp;其最优解$x^\*$，其必然满足如下的必要条件：

##### 1.4.2 Stationarity
&emsp;&emsp;KKT条件的<font color="blue">Stationarity（即平稳性）</font>从最大化和最小化两个优化方向来说，对应如下的式子：
$$
\begin{cases}
最大化f(x)：\nabla f(x^\*) = { \sum\_{i=1}^m \mu\_i \nabla g\_i(x^\*)  + \sum\_{j=1}^l \lambda\_j \nabla h\_j(x^\*) }  \\\\
\\\\
\ 最小化f(x)：-\nabla f(x^\*) = { \sum\_{i=1}^m \mu\_i \nabla g\_i(x^\*)  + \sum\_{j=1}^l \lambda\_j \nabla h\_j(x^\*) }  \\\\
\tag{1 - 1}
\end{cases}
$$

##### 1.4.3 Primal feasibility
&emsp;&emsp;KKT条件的<font color="blue">Primal feasibility（原始可行性）</font>是指原始问题的约束条件：
$$
\begin{cases}
\begin{split}
g_i(x^\*) & \leq 0，i = 1，···，m\\\\
h_j(x^\*) &= 0，j = 1，···，l
\end{split}
\tag{1 - 2}
\end{cases}
$$

##### 1.4.4 Dual feasibility
&emsp;&emsp;KKT条件的<font color="blue">Dual feasibility（对偶可行性）</font>是指不等式约束条件的拉格朗日乘数应满足条件： 

$$
\mu_i \geq 0 ，i = 1，···，m
\tag{1 - 3}
$$

##### 1.4.5 Complementary slackness
&emsp;&emsp;KKT条件的<font color="blue">Complementary slackness（互补松弛）</font>是指如下条件：
$$
\mu_i · g_i(x^\*) = 0 ，i = 1，···，m
\tag{1 - 4}
$$

&emsp;&emsp;上面的$\mu_i、\lambda_j$均称为拉格朗日乘子，**当KKT条件中的$m=0$时，即退化为拉格朗日条件**。
##### 1.4.6 条件整合
&emsp;&emsp;将上述条件整合起来(**求最小化**）就有KKT条件的定义：

$$
\begin{cases}
\begin{split}
-\nabla f(x) &= { \sum\_{i=1}^m \mu\_i \nabla g\_i(x)  + \sum\_{j=1}^l \lambda\_j \nabla h\_j(x) }  \\\\
\mu_i · g_i(x) &= 0 ，i = 1,···，m \\\\
g_i(x) & \leq 0，i = 1, ···, m\\\\
h_j(x) &= 0，j = 1, ···, l \\\\
\mu_i & \geq 0 ，i = 1, ···, m
\end{split}
\end{cases}
$$

&emsp;&emsp;**注意：KKT条件中对等式约束对应的拉格朗日乘子$\lambda \_j$并没有非负性要求！**
#### 1.5 广义拉格朗日函数
##### 1.5.1 表达式
&emsp;&emsp;为了在KKT条件下利用拉格朗日乘子求出最优值，我们需要构造出广义拉格朗日函数(**generalized Lagrange function**)，以上述优化问题为例，其对应的广义拉格朗日函数为：

$$
L(x,\alpha,\beta) = f(x) + { \sum\_{i=1}^k \alpha\_i  g\_i(x) } +  { \sum\_{j=1}^l \beta\_j  h\_j(x) }
\tag{1 - 5}
$$

&emsp;&emsp;式中：

$$
\begin{cases}
\begin{split}
&& \alpha\_i,\beta\_j是拉格朗日乘子; \\\\
&& \alpha\_i \geq 0; 
\end{split}
\end{cases}
$$

##### 1.5.2 极小极大问题
&emsp;&emsp;可以证明（见后文相关部分），若原始问题和对偶问题均存在解，则它们的解是等价的（即有相同的解），上述最优化问题（称为**原始最优化问题**或**原始问题**等价于如下的广义拉格朗日函数的<font color='red'>**极小极大问题**</font>：

$$
\min \limits\_{x} {\theta\_P(x)} = \min \limits\_{x} {\max \limits\_{\alpha , \beta : \alpha\_i \geq 0} { L(x , \alpha , \beta) } }
\tag{ 1 - 6}
$$

&emsp;&emsp;这样就把原始最优化问题转换为了广义拉格朗日函数的极小极大问题。同样为了方便叙述，定义原始最优化问题的解为$p^\*$，即有：

$$
p^\* = \min \limits\_{x} {\theta\_P(x)} = \min \limits\_{x} {\max \limits\_{\alpha , \beta : \alpha\_i \geq 0} { L(x , \alpha , \beta) } }
\tag{ 1 - 7}
$$

#### 1.6 对偶问题
##### 1.6.1 定义
&emsp;&emsp;我们记如下广义拉格朗日函数的<font color='red'>**极大极大小问题（注意极大、极小两个词语的顺序和1.4节的不同）**</font>为上述最优化问题的对偶问题：

$$
\max \limits\_{\alpha , \beta , \alpha\_i \geq 0} {\theta\_D(\alpha , \beta)} = \max \limits\_{\alpha , \beta , \alpha\_i \geq 0} {\min \limits\_{x} { L(x , \alpha , \beta) } }
\tag{ 1 - 8}
$$

&emsp;&emsp;同时，记上述对偶问题的解为$d^\*$，即有：
$$
d^\* = \max \limits\_{\alpha , \beta , \alpha\_i \geq 0} {\theta\_D(\alpha , \beta)} = \max \limits\_{\alpha , \beta , \alpha\_i \geq 0} {\min \limits\_{x} { L(x , \alpha , \beta) } }
\tag{ 1 - 9}
$$

##### 1.6.2 二者关系
&emsp;&emsp;可以证明（见后文相关部分），若原始问题和对偶问题均存在解，则它们的解是等价的，即有：

$$
p^\* = \min \limits\_{x} {\theta\_P(x)} = \max \limits\_{\alpha , \beta , \alpha\_i \geq 0} {\theta\_D(\alpha , \beta)} = d^\*
\tag{ 1- 10}
$$

### 2. 证明详解
#### 2.1 KKT条件的来源
##### 2.1.1 $\alpha\_i · g\_i(x) = 0$来源
&emsp;&emsp;第1节中我们提到了，KKT条件是用来求解带不等式约束的最优化问题的。那不等式约束的优化问题是如何演变出KKT条件的呢？还是先给出原始问题：

$$
\begin{split}
目标&函数：f(x) \\\\
约束&条件： \\\\
\ \ &g_i(x) \leq 0 \\\\
\ \ &h_j(x) = 0
\end{split}
$$

&emsp;&emsp;因为拉格朗日乘子法只适用于无约束或等式约束的情况，所以对带不等式约束的优化问题如果要应用拉格朗日乘子法，我们就得想办法把不等式转换为等式，转换的方法是引入额外的变量，平衡不等式。
&emsp;&emsp;因为$g_i(x) \leq 0$，所以我们只需要引入一个恒非负的变量$s_i$（称之为**松弛变量**），这里还有个小技巧，为了避免引入额外的约束，引入一个具有天然非负特性的项是比较理想的选择，这里我们使用$s^2\_i，s\_i \in R$来实现。
&emsp;&emsp;引入新变量后，我们有：

$$
g'\_i(x, s\_i) = g\_i(x) + s^2\_i= 0
$$

&emsp;&emsp;由于对任意的$g_i(x) \leq 0$，必能找到对应的$s\_i$使上式成立，因此原不等式约束和新的等式约束的作用是完全等价的，即有：

$$
g_i(x) \leq 0 \ \ <==> \ \ g'\_i(x, s\_i)  = 0
$$

&emsp;&emsp;于是原始不等式约束优化问题可以转化为如下的等式约束问题：

$$
\begin{split}
目标&函数：f(x) \\\\
约束&条件： \\\\
\ \ &g'\_i(x, s\_i)  = 0 \\\\
\ \ &h_j(x) = 0
\end{split}
$$

&emsp;&emsp;这个时候就可以使用拉格朗日乘子法构造拉格朗日函数了：

$$
L(x,s\_i,\alpha,\beta) = f(x) + { \sum\_{i=1}^k \alpha\_i  g'\_i(x, s\_i) } +  { \sum\_{j=1}^l \beta\_j  h\_j(x) }
\tag{2 - 1}
$$

&emsp;&emsp;再按照求约束问题的极值的思路，我们有如下方程组：

$$
\begin{cases}
\begin{split}
\frac {\partial L} {\partial x\_i} &= \frac {\partial f} {\partial x\_i}  + { \sum\_{i=1}^k \alpha\_i  \frac {\partial g'\_i(x, s\_i)} {\partial x\_i} } +  { \sum\_{j=1}^l \beta\_j  \frac {\partial h\_j(x)} {\partial x\_i} } \\\\
\
&= \frac {\partial f} {\partial x\_i}  + { \sum\_{i=1}^k \alpha\_i  \frac {\partial g\_i(x)} {\partial x\_i} } +  { \sum\_{j=1}^l \beta\_j  \frac {\partial h\_j(x)} {\partial x\_i} } = 0\\\\
\
\frac {\partial L} {\partial \alpha\_i} &= g'\_i(x, s\_i) = g\_i(x) + s^2\_i = 0\\\\
\
\frac {\partial L} {\partial \beta\_i} &= h\_i(x) = 0 \\\\\
\
\frac {\partial L} {\partial s\_i} &= \alpha\_i  \frac {\partial g'\_i(x, s\_i)} {\partial s\_i} = 2 \alpha\_i · s_i = 0\\\\\
\end{split}
\tag{2 - 2}
\end{cases}
$$

&emsp;&emsp;利用最后一行的等式来简化上述方程组：
&emsp;&emsp;因为：

$$
2 \alpha\_i · s_i = 0 \\\\
$$

&emsp;&emsp;所以我们有：
&emsp;&emsp;&emsp;&emsp;1.$\alpha\_i =0, \  s_i \neq 0 \ ==> \ g\_i(x) < 0(约束条件不作用于目标函数)$;
&emsp;&emsp;&emsp;&emsp;2.$\alpha\_i \neq 0, \  s_i = 0 \ ==> \ g\_i(x) = 0(约束条件作用于目标函数)$;
&emsp;&emsp;&emsp;&emsp;3.$\alpha\_i =0, \  s_i = 0 \ ==> \ g\_i(x) = 0(约束条件不作用于目标函数)$;
&emsp;&emsp;整合三种情况可得到：

$$
\alpha\_i · g\_i(x) = 0
$$

##### 2.1.2 $\alpha\_i \geq 0$来源

#### 2.1 KKT条件的理解
&emsp;&emsp;上面1.3节列出了KKT条件，式（1 - 1）的Stationarity条件看懂了吗？光看这个式子，让人对这个条件有点云里雾里，我们结合图形来说明就容易理解了。为简单起见，我们以三维坐标系为例，设函数为$z = f(x , y)$。

##### 2.1.1 函数等高线
&emsp;&emsp;假如我们将给定的函数式$z =  f(x , y)$绘制在三维平面坐标系中，该函数对应的图形为曲面（平面属于特殊曲面），当我们取特定的z值如$z = f(x , y) = 2$时，则是将该曲面上所有z坐标为2的点取出来，这些点构成的是一条曲线，我们注意到这条曲线上的所有点都有相同的z值，也即相同的“高”，因此我们称曲线$f(x , y) = c$为函数$f(x , y)$的**等高线**，如下草图所示：
<div style="text-align:center"  ><img src="拉格朗日对偶问题/函数及函数等高线.jpg"width="400" height="300" /> <i align='center'>图2-1　　函数及函数等高线</i> 
</div>

&emsp;&emsp;由图我们可知，如果$f(x , y) \in R^+$，那么函数的等高线是有无数条的。
##### 2.1.2 等高线的应用
&emsp;&emsp;介绍了等高线，那么有什么用呢。最优化问题可以转换为等高线之间的关系问题，下面我们先以一个最简单的最优化问题来说明，给定如下最优化问题：

$$
\begin{cases}
\min \limits\_{} { f(x , y) } \\\\
\begin{split}
\textbf{s.t}\ \ g(x , y) = c_1
\end{split}
\tag{ 2 - 1}
\end{cases}
$$

&emsp;&emsp;我们把上式（2 - 1）中的$f(x , y)、g(x , y) = c_1$绘制在三维坐标系中，可知前者是一个曲面，后者是另一个曲面上的一条等高线，如下草图所示：
<div style="text-align:center"  > <img src="拉格朗日对偶问题/最优化和等高线关系1.jpg" width="400" height="300" /> <i align='center'>图2-2　　最优化函数的三维图</i> 
</div>

&emsp;&emsp;原最优化问题用语言表达出来就是：在所有满足$g(x , y) = c_1$的点中，找出一个（或几个）点，使得$f(x , y)$最大。又因为$f(x , y)和g(x , y)$具有相同的自变量（$x、y$），因此从几何意义上来讲，就是将曲线$g(x , y) = c_1$上的点投影到曲面$f(x , y)$上，形成投影曲线$g'(x , y)$，然后找出该投影曲线上坐标值z最小的点，如下图所示：
<div style="text-align:center"  > <img src="拉格朗日对偶问题/最优化和等高线关系2.jpg"  width="400" height="300" /> <i align='center'>图2-3　　最优化函数的三维图2</i> 
</div>

&emsp;&emsp;<font color='red'>注意：曲线$g'(x , y)$只是曲面$f(x , y)$上的一条普通曲线，并不一定满足等高线特性。</font>
&emsp;&emsp;进一步，如果我们把曲面$f(x , y)$的所有等高线和约束等高线$g(x , y) = c_1$投影到X-Y平面上，那么最优化问题实际上就是找到曲面$f(x , y)$的所有等高线中与曲线$g(x , y) = c_1$<font color='red'>**相切**</font>的那条，<font color='red'>为什么不是相交？</font>因为相交就意味着还有其它等高线在这条等高线的内部或者外部与约束曲线相交，从而使得目标函数的值更小或更大，如下图所示：
<div style="text-align:center"  > <img src="拉格朗日对偶问题/最优化和等高线关系3.jpg"  width="400" height="300" /><i align='center'>图2-4　　等高线和约束曲线投影</i> 
</div>

&emsp;&emsp;如上图2-4所示，为了方便查看，我只画了三条目标函数的等高线投影，正如前面所述，并不代表我们的目标函数等高线只有三条！图中约束曲线和目标函数的所有等高线的交点$d_1、d_2、d_4、d_5$都不可能是最优化问题的解，只有$d_3$有可能成为最优值。

##### 2.1.3 梯度
&emsp;&emsp;梯度是微积分里面多元函数对各个变量求偏导后的表达形式，比如二元函数$f(x,y)的梯度为：grad(x,y)或者\nabla(x,y) =  (\frac {\partial f}{\partial x} , \frac {\partial f} {\partial y})$，梯度的几何意义是：函数**增长最快**的地方。用一座山坡来理解的话，梯度对应的就是山坡最为陡峭的地方。
&emsp;&emsp;梯度向量就是由函数对各个变量求偏导后的值组成的向量，**沿着梯度向量方向$\nabla f(x\_1, x\_2, ... , x\_n)$因为函数增加的最快（山势更陡峭），所以更容易到达函数的最大值处；相反，沿着梯度向量的反方向$-\nabla f(x\_1, x\_2, ... , x\_n)$函数增加的最慢（山势更平缓），则更容易到达函数的极小值处。**

##### 2.1.4 梯度与最优化
&emsp;&emsp;光知道目标函数等高线和约束曲线相切还无法完全确定是最大值还是最小值，比如上图的点$d_3$，只看图2-4的话是无法下结论的，因为点$d_3$所在的等高线的Z值完全有可能比点$d_4$的大，从而不可能是最小值。这也是为什么地形图上不但要作出等高线，还要标注好高程。
&emsp;&emsp;如果不标注高程，那么图2-4所表示的山，既可能是上凸的（对应山尖），也可能是下凹的（对应山坳）。那么函数等高线又如何区分呢？答案就是：<font color='red'>**梯度**</font>！梯度大的地方对应山尖，梯度小的地方对应山坳。
&emsp;&emsp;对任意函数$f(x)$，其梯度可表示为：

$$
\nabla f(x) =  (\frac {\partial f}{\partial x\_1} , \frac {\partial f} {\partial x\_2}, ···, \frac {\partial f}{\partial x\_k}) ，k \in N^+
\tag{2 - 2}
$$

&emsp;&emsp;<font color='red'>**目标函数等高线和约束曲线的切点（候选最值点），它在目标函数上的梯度向量和约束函数上的梯度向量一定共线！**</font>即有：
$$
\begin{split}
&最大值：\nabla f(x_0) =  \lambda  \nabla g(x_0)，\lambda \in R^+ \\\\
&最小值：-\nabla f(x_0) =  \lambda  \nabla g(x_0)，\lambda \in R^+
\end{split}
\tag{2 - 3}
$$

&emsp;&emsp;式中，$x_0$表示切点。上式我们称为**梯度共线公式**，<font color="red" size=3>**由上式可知，梯度共线公式与约束条件的常数项（式2-1中的$c_1$）无关！**</font>增加等式约束条件，我们就有多等式约束条件下的梯度共线公式：
$$
\begin{split}
&最大值：\nabla f(x_0) = \lambda_1  \nabla g_1(x_0) + \lambda_2  \nabla g_2(x_0) + ··· + \lambda_k  \nabla g_k(x_0)，k = 2,··· \\\\
&最小值：-\nabla f(x_0) = \lambda_1  \nabla g_1(x_0) + \lambda_2  \nabla g_2(x_0) + ··· + \lambda_k  \nabla g_k(x_0)，k = 2,···
\end{split}
\tag{2 - 4}
$$
&emsp;&emsp;上式（2-4）是不是跟我们的Stationarity条件表达式（1-1）形式上一模一样了！但也仅仅只是形式上，还差那么一点点，因为式（1-1）是含有不等式约束的梯度共线公式，而式（2-4）是针对所有约束条件为等式的情况。
&emsp;&emsp;对于含不等式约束的情况，与等式约束情况的结果是一样的，只不过如图（2-4）所示，约束条件投影到$XoY$平面的是一块区域而不只是一条单独的曲线。而等式约束条件$g_i(x)=c_i$是这块投影区域的边界线中的一条（内边界线或外边界线）。在这种情况下求出来的梯度共线公式仍然是上式（2-4）。
