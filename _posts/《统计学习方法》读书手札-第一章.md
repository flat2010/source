---
title: 《统计学习方法》读书手札-第一章
date: 2014-06-20 16:35:51
tags: 统计学习 读书笔记
categories: 读书笔记 
comments: true 
---
![image](《统计学习方法》读书手札-第一章/封面.png)
><font color=#0000FF face="微软雅黑" size=4>Every oak must be an acorn.</font>
***  

## 一、学习分类
&emsp;&emsp;统计学习主要由以下几种组成：  
&emsp;&emsp;  ①监督学习（Supervised learning）；  
&emsp;&emsp;  ②非监督学习（Unsupervised learning）；  
&emsp;&emsp;  ③半监督学习（Semi-supervised learning）；
&emsp;&emsp;  ④强化学习（Reinforcement learning）。  
&emsp;&emsp;在《统计学习方法》一书中，主要围绕**①监督学习（Supervised learning）**进行。
## 二、三要素
&emsp;&emsp;统计学习方法由如下三要素构成：  
&emsp;&emsp;①模型（model）；  
&emsp;&emsp;②策略（strategy）；  
&emsp;&emsp;③算法（algorithm）。  
&emsp;&emsp;统计学习方法可以简单的表述成：  
```shell
方法 = 模型 + 策略 + 算法
```

### 2.1 模型
&emsp;&emsp;**模型（model）就是所要学习的条件概率分布或决策函数。**其中由条件概率表示的模型称为**`概率模型`**，而由决策函数表示的模型则称为**`非概率模型`**。 
&emsp;&emsp;**模型的本质**是一个从输入到输出的**映射**。给定输入、输出数据的情况下，满足映射关系的模型是非常多的，这些映射关系所组成的集合就称为**`假设空间(hypothesis space)`**，而学习的目的则在于从假设空间中寻找到**最优**的模型。  

#### 2.1.1 决策函数
&emsp;&emsp;以决策函数的集合定义的假设空间表达式为：  
![image](《统计学习方法》读书手札-第一章/决策函数表示的假设空间.png)
<div align='center'>图2-1　　决策函数表示的假设空间</div>

&emsp;&emsp;式中：  

&emsp;&emsp;&emsp;&emsp;X——是定义在输入空间上的变量；  
&emsp;&emsp;&emsp;&emsp;Y——是定义在输出空间上的变量。  

&emsp;&emsp;通常假设空间是由一个**`n(n ∈ R^n)`**维的参数向量θ控制的，称为**`参数空间(parameter space)`**。

#### 2.1.2 条件概率
&emsp;&emsp;以条件概率的集合定义的假设空间表达式为（式中X、Y含义同上）：  
![image](《统计学习方法》读书手札-第一章/条件概率表示的假设空间.png)
<div align='center'>图2-2　　条件概率表示的假设空间</div>

### 2.2 策略
&emsp;&emsp;所谓**`策略`**是我们从假设空间选取最优模型的学习准则或者学习方法。包含**`损失函数`**、**`风险函数`**、**`结构函数`**。  

#### 2.2.1 损失函数
&emsp;&emsp;**`损失函数(loss function)`**（也称作**`代价函数(cost function)`**），用来评估模型<font color=#FF0000 size=4>单次</font>预测的好坏。它是模型`f(x)`和输出`Y`的<font color=#FF0000 size=4>非负实值</font>函数，记作：**`L(Y , f(X))`**。常用损失函数如下：  

&emsp;&emsp;**①0 — 1损失函数(0-1 loss function) **   

![image](《统计学习方法》读书手札-第一章/0-1损失函数.png)
<div align='center'>图2-3　　0—1损失函数</div>

&emsp;&emsp;**②平方损失函数(quadratic loss function) **   

![image](《统计学习方法》读书手札-第一章/平方损失函数.png)
<div align='center'>图2-4　　平方损失函数</div>  

&emsp;&emsp;**③绝对损失函数(absolute loss function) **      

![image](《统计学习方法》读书手札-第一章/绝对损失函数.png)
<div align='center'>图2-5　　绝对损失函</div>  

&emsp;&emsp;**④对数损失函数(logarithmic loss function) **  或 **对数似然损失函数(log-likelihood loss function)**     

![image](《统计学习方法》读书手札-第一章/对数损失函数.png)
<div align='center'>图2-6　　对数损失函数</div>  

#### 2.2.2 风险函数 VS 经验风险
&emsp;&emsp;所谓**`风险函数(Risk Function)`**也称作**`期望损失(Expected Loss)`**，是对损失函数求期望，它是关于**联合分布`P(X,Y)`**的期望损失，**本质仍然是期望**，定义式为：   
![image](《统计学习方法》读书手札-第一章/风险函数定义.png)
<div align='center'>图2-7　　风险函数定义</div>  

&emsp;&emsp;由上式可知，损失函数`L(y,f(x))`我们可以自己选择，但要求得模型的风险值，还需要知道输入`X`和输出`Y`两个随机变量的联合分布`P(X,Y)`，而这个联合分布又是未知的，所以直接求解风险函数是不现实的，通常是通过求解**经验风险**来实现模型评估的。  
&emsp;&emsp;所谓**`经验风险(Empirical Risk)`**也称作**`经验损失(Empirical Loss)`**，是对损失函数求平均值，它是关于**训练样本集**的平均损失，**本质是均值**，定义式为：  
![image](《统计学习方法》读书手札-第一章/经验风险函数定义.png)
<div align='center'>图2-8　　经验风险函数定义</div>  

&emsp;&emsp;式中`(xi,yi)`表示第i个训练样本，`N`则表示训练集中样本总数，由大数定律可知，当训练样本总数`N`趋近于<font color=#000000 size=4>∞</font>时，经验风险**R<font color=#000000 size=1>emp</font>(f)**趋近于期望风险**R<font color=#000000 size=1>exp</font>(f)**。也即当我们手里有大量训练样本时，可以使用经验风险去估计期望风险，但是现实中往往训练样本数量有限，这个时候再使用经验风险去估计就会出现较大偏差，使得模型准确性较低，这个时候就需要对经验风险进行矫正，结构风险就应运而生了。  

#### 2.2.3 过拟合 VS 结构风险
&emsp;&emsp; 所谓**过拟合(Over-fitting)** ，是指我们根据已有数据建立的模型过于复杂，以至于该模型在已有数据上表现的非常完美而在新的数据集上却非常差，也即缺乏**泛化能力**。过拟合现象如下图所示：    
![image](《统计学习方法》读书手札-第一章/过拟合示意图.png)
<div align='center'>图2-9　　过拟合示意图</div>  

&emsp;&emsp; 上图中黑色曲线是加入了正则化项的较好的拟合模型，而绿色曲线则是过拟合模型，可以看出两个模型中，过拟合模型对已有数据的拟合是相当完美的，成功的区分了每一个红色和蓝色圆点。但是当我们把这个模型用到新的数据集上时，新的样本稍微有点抖动，过拟合模型就会分类错误。对于噪声数据，也会相当敏感。
&emsp;&emsp;所谓 **结构风险(Structural Risk)**是在**经验风险**的基础上加了一个用来评估模型复杂度的**正则化项(Regularizer)**也称作**罚项(Penalty Term)**，用来约束模型的复杂度，其定义为：  
![image](《统计学习方法》读书手札-第一章/结构风险函数定义.png)
<div align='center'>图2-10　　结构风险函数定义</div>  

&emsp;&emsp;图中红色部分即为正则化项或罚项，其中**`J(f)`**为模型的复杂度函数，正比于模型复杂度。**`λ(≥0)`**为调节系数，用来调节经验风险和模型负责度。  

#### 2.2.4 经验风险最小化 VS 结构风险最小化
&emsp;&emsp; 顾名思义，**`经验风险最小化(Empirical Risk Minimization)-简称ERM`**是对经验风险函数求最小值。因为经验风险正比于损失函数，所以最小化经验风险即是最小化损失，也就是最小化模型的误差或者说**最大化模型的精度**。按照ERM策略求解模型即是求：    
![image](《统计学习方法》读书手札-第一章/经验风险最小化定义.png)
<div align='center'>图2-11　　经验风险最小化定义</div>  

&emsp;&emsp;前已述及，ERM策略适合样本容量较大的情况。ERM的一个典型例子是：**`极大似然估计(Maximum Likelihood Estimation)`**，它是**模型为条件概率分布**，**损失函数取对数损失**的情况。  
&emsp;&emsp;样本较少时，ERM策略选择出的模型并非是最优的，因为它通常会导致训练模型复杂化（过拟合）。为了控制这种风险，引入了结构风险最小化。  
&emsp;&emsp;顾名思义，**`结构风险最小化(Structural Risk Minimization)-简称SRM`**是对结构函数求最小值。因为结构函数中的正则化项正比于模型复杂度，所以最小化结构风险相当于平衡了模型复杂度和经验风险，使模型**泛化能力和准确率均最优**。按照SRM策略约束模型即是求：    
![image](《统计学习方法》读书手札-第一章/结构风险最小化定义.png)
<div align='center'>图2-12　　结构风险最小化定义</div>    

#### 2.2.5 策略小结
&emsp;&emsp; 最后，用一张图来简单说明上述概念之间的关系，如下所示：  
![image](《统计学习方法》读书手札-第一章/策略小结.png)
<div align='center'>图2-13　　策略小结</div>   

### 2.3 算法
&emsp;&emsp;所谓**算法**是指模型具体的计算方法，主要是求解最优化问题的算法。它要解决两个问题：一是**高效**，二是**全局最优**。这里先做个引子，后面会有大量篇幅讲解算法。





